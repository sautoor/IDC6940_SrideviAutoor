@article{yan2025synthetic,
  title={Synthetic data for enhanced privacy: A VAE-GAN approach against membership inference attacks},
  author={Yan, Jian’en and Huang, Haihui and Yang, Kairan and Xu, Haiyan and Li, Yanling},
  journal={Knowledge-Based Systems},
  volume={309},
  pages={112899},
  year={2025},
  publisher={Elsevier}
}


@misc{larsen_autoencoding_2016,
	title = {Autoencoding beyond pixels using a learned similarity metric},
	url = {http://arxiv.org/abs/1512.09300},
	doi = {10.48550/arXiv.1512.09300},
	abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
	urldate = {2025-07-19},
	publisher = {arXiv},
	author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
	month = feb,
	year = {2016},
	note = {arXiv:1512.09300 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}


@misc{noauthor_reparameterization_nodate,
	title = {The {Reparameterization} {Trick}},
	urldate = {2025-07-19}
}

@inproceedings{razghandi2022variational,
  title={Variational autoencoder generative adversarial network for synthetic data generation in smart home},
  author={Razghandi, Mina and Zhou, Hao and Erol-Kantarci, Melike and Turgut, Damla},
  booktitle={ICC 2022-IEEE International Conference on Communications},
  pages={4781--4786},
  year={2022},
  organization={IEEE}
}

@inproceedings{zhu2019brief,
  title={A brief review on cycle generative adversarial networks},
  author={Zhu, Miaomiao and Gong, Shengrong and Qian, Zhenjiang and Zhang, Lifeng},
  booktitle={The 7th IIAE international conference on intelligent systems and image processing (ICISIP)},
  pages={235--242},
  year={2019}
}


@misc{noauthor_variational_2023,
	title = {Variational autoencoders},
	abstract = {Variational autoencoders (VAEs) are a family of deep generative models with use cases that span many applications, from image processing to bioinformatics. There are two complimentary ways of viewing the VAE: as a probabilistic model that is fit using variational Bayesian inference, or as a type of autoencoding neural network. In this post, we present the mathematical theory behind VAEs, which is rooted in Bayesian inference, and how this theory leads to an emergent autoencoding algorithm. We also discuss the similarities and differences between VAEs and standard autoencoders. Lastly, we present an implementation of a VAE in PyTorch and apply it to the task of modeling the MNIST dataset of hand-written digits.},
	language = {en},
	urldate = {2025-07-19},
	journal = {Matthew N. Bernstein},
	month = mar,
	year = {2023}
}


@misc{sharma_deep_2023,
	title = {A {Deep} {Dive} into {Variational} {Autoencoders} with {PyTorch}},
	abstract = {Explore Variational Autoencoders: Understand basics, compare with Convolutional Autoencoders, and train on Fashion-MNIST. A complete guide.},
	language = {en-US},
	urldate = {2025-07-19},
	journal = {PyImageSearch},
	author = {Sharma, Aditya},
	month = oct,
	year = {2023}
}


@online{weng2018vae,
  author = {Weng, Lilian},
  title = {From Autoencoder to {Beta-VAE}},
  date = {2018-08-12}
}

@article{li2023comprehensive,
  title={A comprehensive survey on design and application of autoencoder in deep learning},
  author={Li, Pengzhi and Pei, Yan and Li, Jianqiang},
  journal={Applied Soft Computing},
  volume={138},
  pages={110176},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}
@article{henry2021pix2pix,
  title={Pix2pix gan for image-to-image translation},
  author={Henry, Joyce and Natalie, Terry and Madsen, Den},
  journal={Research Gate Publication},
  pages={1--5},
  year={2021}
}
@inproceedings{zhu2017unpaired,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2223--2232},
  year={2017}
}
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{sharma2024novel,
  title={A Novel Method for Bi-directional Translation of MRI and CT Images—Variational Autoencoder CycleGAN},
  author={Sharma, Yashvi and Diya and Shikha and Naqvi, Najme Zehra},
  booktitle={International Conference on Data Analytics \& Management},
  pages={329--340},
  year={2024},
  organization={Springer}
}
@inproceedings{zhai2018autoencoder,
  title={Autoencoder and its various variants},
  author={Zhai, Junhai and Zhang, Sufang and Chen, Junfen and He, Qiang},
  booktitle={2018 IEEE international conference on systems, man, and cybernetics (SMC)},
  pages={415--419},
  year={2018},
  organization={IEEE}
}

@misc{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max and others},
  year={2013},
  publisher={Banff, Canada}
}


@misc{noauthor_peak_2025,
	title = {Peak signal-to-noise ratio},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Peak_signal-to-noise_ratio&oldid=1288083763},
	abstract = {Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.
PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.},
	language = {en},
	urldate = {2025-08-03},
	journal = {Wikipedia},
	month = apr,
	year = {2025},
	note = {Page Version ID: 1288083763},
}

@misc{noauthor_mean_2025,
	title = {Mean squared error},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Mean_squared_error&oldid=1289884489},
	abstract = {In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the true value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution).
The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.
The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.},
	language = {en},
	urldate = {2025-08-03},
	journal = {Wikipedia},
	month = may,
	year = {2025},
	note = {Page Version ID: 1289884489},
}


@misc{noauthor_structural_2025,
	title = {Structural similarity index measure},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Structural_similarity_index_measure&oldid=1284192447},
	abstract = {The structural similarity index measure (SSIM) is a method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos.  It is also used for measuring the similarity between two images. The SSIM index is a full reference metric; in other words, the measurement or prediction of image quality is based on an initial uncompressed or distortion-free image as reference.
SSIM is a perception-based model that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms.  This distinguishes from other techniques such as mean squared error (MSE) or peak signal-to-noise ratio (PSNR) that instead estimate absolute errors.  Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in the visual scene. Luminance masking is a phenomenon whereby image distortions (in this context) tend to be less visible in bright regions, while contrast masking is a phenomenon whereby distortions become less visible where there is significant activity or "texture" in the image.},
	language = {en},
	urldate = {2025-08-03},
	journal = {Wikipedia},
	month = apr,
	year = {2025},
	note = {Page Version ID: 1284192447},
}

@misc{noauthor_frechet_2025,
	title = {Fréchet inception distance},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Fr%C3%A9chet_inception_distance&oldid=1302675336},
	abstract = {The Fréchet inception distance (FID) is a metric used to assess the quality of images created by a generative model, like a generative adversarial network (GAN) or a diffusion model. 
The FID compares the distribution of generated images with the distribution of a set of real images (a "ground truth" set).  Rather than comparing individual images, mean and covariance statistics of many images generated by the model are compared with the same statistics generated from images in the ground truth or reference set.  A convolutional neural network such as an inception architecture is used to produce higher-level features describing the images, thus leading to the name Fréchet inception distance.
The FID is inspired by the earlier inception score (IS) metric which evaluates only the distribution of generated images. The FID metric does not replace the IS metric; classifiers that achieve the best (lowest) FID score tend to have greater sample variety while classifiers achieving the best (highest) IS score tend to have better quality within individual images.
The FID metric was introduced in 2017, and is the current standard metric for assessing the quality of models that generate synthetic images as of 2024.  It has been used to measure the quality of many recent models including the high-resolution StyleGAN1 and StyleGAN2 networks, and diffusion models.
The FID attempts to compare images visually through deep layers of an inception network. More recent works take this further by instead comparing CLIP embeddings of the images.},
	language = {en},
	urldate = {2025-08-03},
	journal = {Wikipedia},
	month = jul,
	year = {2025},
	note = {Page Version ID: 1302675336},
}

