---
title: "VAE-CycleGAN"
subtitle: "A Cycle Consistent Probabilistic Framework for Unpaired Image Translation"
author: "Sridevi Autoor <br><br>Advisor: <br> Dr. Achraf Cohen "
date: '`r Sys.Date()`'
format:
  html:
    number-sections: false
    number-tables: false
    crossref: 
      eq-prefix: "Eq:"
    code-fold: true
engine: knitr  # Forces R as the default engine    
jupyter: python3  # Uses Python (requires Jupyter kernel)
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72


---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

<style>
  .code-toggle {
    margin: 10px 0;
  }
  .code-toggle-btn {
    background: #f0f0f0;
    border: 1px solid #ddd;
    padding: 5px 10px;
    cursor: pointer;
    font-family: monospace;
  }
  .code-toggle-btn::after {
    content: " ▼";
  }
  .code-toggle-btn.collapsed::after {
    content: " ►";
  }
  .code-content {
    display: block;
  }
  .code-content.collapsed {
    display: none;
  }
  

</style>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.code-toggle-btn').forEach(btn => {
      btn.addEventListener('click', () => {
        const content = btn.nextElementSibling;
        btn.classList.toggle('collapsed');
        content.classList.toggle('collapsed');
      });
    });
  });
</script>

## 1. Introduction

Image-to-image translation is a fundamental task in computer vision, enabling applications such as style transfer, domain adaptation, and photo enhancement. The goal is to learn a mapping function that transforms an input image from a source domain (e.g., daytime photos) into a corresponding output in a target domain (e.g., nighttime photos). While recent advances in deep generative models have significantly improved translation quality, key challenges remain—particularly in unpaired settings, where aligned training data is unavailable, and in controllable generation, where users desire fine-grained manipulation of outputs.


### 1.1 Related Work

Deep generative models have traditionally struggled with intractable probabilistic computations, especially in maximum likelihood estimation, and have found it difficult to effectively use piece wise linear units in generative settings. Early models such as Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs) depend on computationally expensive Markov Chain Monte Carlo (MCMC) methods, while Variational Autoencoders (VAEs) require approximate inference. Other techniques, like score matching and noise-contrastive estimation, impose strict constraints on how probability densities are specified.

Generative Adversarial Networks (GANs)[@goodfellow2014generative] address many of these challenges through an adversarial training framework. They eliminate the need for Markov chains, explicit likelihoods, or approximate inference. By framing learning as a minimax game between a generator ($G$) and discriminator ($D$), GANs learn a loss that tries to classify if the output image is real or fake, while simultaneously training a generative model to minimize this loss. GANs enable efficient backpropagation-based optimization while producing high-fidelity samples.

Though GANs are powerful for generating realistic data, they suffer from a few drawbacks:

1. Mode Collapse: the generator may produce limited varieties of outputs instead of covering the full data distribution. For ex: We get the same painting output for a variety of input photos. 

2. Non-convergence: the competition between the generator ($G$) and discriminator ($D$) may never stabilize, causing persistent oscillation in losses.

3. Requires paired data: GAN-based approaches such as conditional GANs [@isola2017image], requires  paired examples ($x$, $y$) where the generator learns a mapping from input image $x$ to generate the output image $y$. However, obtaining such paired datasets is often impractical in real-world scenarios.


To overcome these limitations, unsupervised image-to-image translation models such as CycleGAN [@zhu2017unpaired] have been developed which employ two generator-discriminator pairs ($G:X→Y$ and $F:Y→X$) and implement cycle-consistency loss ($F(G(X)) ≈ X$ and vice versa) along with adversarial losses to enable stable unpaired image translation. Nevertheless, CycleGAN has its own drawbacks:

1. There’s no explicit latent space within the generator (or between the two generators) and therefore there is no control over output. The mapping between domains is learned implicitly by the generators. We can't easily specify style, color, or semantic features of the output. A vanilla encoder-decoder CycleGAN (without VAE) is just a deterministic autoencoder-style model. For example, given an  image of a horse, it will always generate the same image of a zebra.

2. There's limited output diversity. Deterministic mappings in the latent space within a generator inhibit multimodal generation. For example, translating a cloudy sky to multiple sunset styles is not possible.  

3. For a given number of $n$ distinct inputs ($\forall x_i \in X,\ i \in \{1,\ldots,n\}$) that belong to the $X$ domain, the model may fail to translate/generate a single output ($y_i \in Y,\ i \in \{1,\ldots,n\}$) out of the possible $n!$ permutations (more in the *Methods Convergence* section).  

A hybrid framework that integrates Variational Autoencoders (VAEs) into a GAN was developed by [@yan2025synthetic]  to leverage the strengths of both architectures.

As explained by [@larsen_autoencoding_2016], by combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. 

Recent studies[@yan2025synthetic] demonstrates that the VAE component, utilizing its encoder–decoder architecture, plays a crucial role in enhancing the model’s ability to generalize by learning a latent space representation of the data. This latent space facilitates the generation of high-quality synthetic data by capturing underlying data distributions. However, VAE alone can sometimes deviate from the real data distribution, which may affect the utility of the synthetic data. 

To address this limitation, GAN was introduced into the model framework. GAN’s adversarial training process further refines the generator’s output by continuously optimizing against a discriminator. This adversarial mechanism ensures that the generated data remains close to the real data distribution while simultaneously improving the generator’s ability to produce high-fidelity synthetic data. 

The combination of VAE and GAN thus leverages the strengths of both approaches: VAE’s robust feature extraction and GAN’s fine-tuning through adversarial feedback, leading to enhanced data desensitization, robustness and generalization.

We propose an extension of VAE-GAN, VAE-CycleGAN, another hybrid framework that integrates Variational Autoencoders (VAEs) into CycleGAN. Incorporating a VAE into CycleGAN (or replacing parts of CycleGAN with VAE components) combines the adversarial and cycle-consistent training of CycleGAN with the probabilistic latent space technique of VAEs. The advantages of VAE-CycleGAN over the standard CycleGAN or a simple encoder-decoder CycleGAN are:

1. Instead of a fixed latent code within the generator, the VAE encodes inputs as mini distribution means, enabling sampling. A VAE-based CycleGAN enforces a structured latent distribution (e.g., Gaussian) via the Kullback-Leibler divergence (KL) loss, making interpolation and manipulation easier (e.g., for attribute editing). 

2. While CycleGAN deterministically maps one input to one output, a VAE can model multimodality by sampling different latent codes $z$ and decode them back to generate different outputs, thus capturing uncertainty and variability in the data. For ex: The generator can produce multiple zebra images (may be different stripe patterns, change in backgrounds, poses etc.) for the same photo of a horse.

By unifying VAEs and CycleGAN, this project develops a framework for bi-directional translation, where the model:

1. learns to map images from a source domain ($X$, e.g., pictures) to a target domain ($Y$, e.g., paintings) without requiring paired training data

2. enforces a probabilistic latent via the KL divergence loss 

3. ensures realistic and reversible translations (via CycleGAN) with adversarial and cycle-consistency 

4. preserves input fidelity during domain transfers with VAE reconstruction loss.  


## 2 Methods

### 2.1 Autoencoder

As mentioned by [@li2023comprehensive], an autoencoder is an unsupervised learning model, which can automatically learn data features from a large number of samples and can act as a dimensionality reduction method. An autoencoder consists of an encoder, which compresses input data into a lower-dimensional representation, and a decoder, which reconstructs the original input. By minimizing reconstruction error, the model learns efficient, compact embeddings—often used for dimensionality reduction or as features for other machine learning model.


::: {.figure}
![](images/autoencoder-architecture.png){width=100%}

<div style="text-align: center; position: relative;">
  <span class="caption">Figure 1: Autoencoder</span>
  <span style="position: absolute; right: 0;">*Source:* [@weng2018vae] </span>
</div>
:::

As [@weng2018vae] explains, the model contains an encoder function $g(.)$ parameterized by $\phi$ and a decoder function $f(.)$ parameterized by $\theta$. The low-dimensional code learned for input $\mathbf{x}$ in the bottleneck layer is $\mathbf{z} = g_\phi(\mathbf{x})$ and the reconstructed input is $\mathbf{x}' = f_\theta(g_\phi(\mathbf{x}))$

The parameters $(\theta, \phi)$ are learned together to output a reconstructed data sample same as the original input, $\mathbf{x} \approx f_\theta(g_\phi(\mathbf{x}))$, or in other words, to learn an identity function. There are various metrics to quantify the difference between two vectors, such as cross entropy when the activation function is sigmoid, or as simple as MSE loss:
$$
L_\text{AE}(\theta, \phi) = \frac{1}{n}\sum_{i=1}^n (\mathbf{x}^{(i)} - f_\theta(g_\phi(\mathbf{x}^{(i)})))^2
$$

- $L_\text{MSE}(\theta ,\phi)$ represents the reconstruction loss.
- $\theta$ and $\phi$ are the parameters of the decoder and encoder, respectively.
- n is the number of samples.
- $x^{(i)}$ is the original input image.
- $f_\theta$ is the decoder function, and $g_\phi$ is the encoder function.

### 2.2 Variational Auto-Encoder (VAE)

Autoencoders(AE) are useful for data compression, denoising or feature extraction. These are deterministic models where the input is mapped to a deterministic latent vector. Given the same input, the encoder will always produce the same point in the latent space. AE cannot generate new data due to the absence of regularity in the latent space which results in lack of continuation in interpolation of data points not in the input sequence. 

To address this issue, Variational Autoencoders (VAEs) offer a solution by imposing additional constraints on the latent space. The idea of Variational Autoencoder[@kingma2013auto], short for VAE, is actually less similar to the autoencoder model above, but deeply rooted in the methods of variational bayesian and graphical model.

VAEs ensure continuous latent spaces, facilitating random sampling and interpolation, making them invaluable for generative modeling. This continuity ensures that small changes in the latent space result in coherent changes in the generated data, making VAEs suitable for tasks like interpolation between data points. Additionally, the probabilistic nature of VAEs introduces a level of randomness that can benefit generative tasks, allowing the model to produce diverse outputs.


VAEs are probabilistic generative models of independent, identically distributed samples, ${x_1,…,x_n}$. In this model, each sample, $x_i$, is associated with a latent (i.e. unobserved), lower-dimensional variable $z_i$. Variational autoencoders are a generative model in that they describe a joint distribution over samples and their associated latent variable, $p(x,z)$


::: {.figure}
![](images/VAE_as_autoencoder.png){width=100%}

<div style="text-align: left; position: relative;">
  <span class="caption">Figure 2: Variational Autoencoder</span>
  <span style="position: absolute; right: 0;">*Source:* [@noauthor_variational_2023] </span>
</div>
:::

The conditional probability $f_\theta(\mathbf{x} \vert \mathbf{z})$ defines a generative model, similar to the decoder $f_\theta(\mathbf{x} \vert \mathbf{z})$  introduced above in Figure1. $f_\theta(\mathbf{x} \vert \mathbf{z})$  is also known as probabilistic decoder.
The approximation function $h_\phi(\mathbf{z} \vert \mathbf{x})$  is the probabilistic encoder, playing a similar role as $g_\phi(\mathbf{z} \vert \mathbf{x})$ above in Figure 1.
 
### 2.3 Reparameterization Trick

The encoder produces a Gausssian distribution’s parameters (mean and variance), and the actual latent representation is sampled from this distribution. The essence of the trick lies in introducing an auxiliary random variable, typically drawn from a standard normal distribution.
 
::: {.figure}
![](images/reparam-vae.jpg){width=100%}

<div style="text-align: center; position: relative;">
  <span class="caption">Figure 3: Reparameterization Trick</span>
  <span style="position: absolute; right: 0;">*Source:* [@sharma_deep_2023] </span>
</div>
:::

Mathematically, the latent distribution can be represented as:

$$Z = Z_\mu + Z_\sigma \odot \varepsilon$$

Here, $\varepsilon$ is sampled from a standard normal distribution, that is, $\varepsilon \sim \mathcal{N}(0, 1)$. The symbol $\odot$ stands for element-wise multiplication.

### 2.4 Approximate the intractable posterior:

VAE approximates $p_{\theta}(x)$ by introducing a variational lower bound. Given the latent variable $z$, VAE attempts to find the model parameter $\theta$ by maximum likelihood method. Due to the latent variable $z$ affecting data $x$, maximum a posteriori with a prior knowledge of $z$ must be considered instead of maximum likelihood. Specifically, VAE estimates posteriori probability $p(z|x)$ with an assumption of a prior knowledge $p(z)$ being a normal Gaussian distribution and drives the approximating model $Q_{\phi}(z\vert x)$ to approximate real (otherwise, intractable) posteriori probability $p(z|x)$.

::: {.figure}
![](images/VAE-graphical-model.png){width=100%}

<div style="text-align: center; position: relative;">
  <span class="caption">Figure 4: VAE graphical model</span>
  <span style="position: absolute; right: 0;">*Source:* [@weng2018vae]</span>
</div>
:::

Intuitively, $Q_{\phi}(z\vert x)$ is an encoder which generates the latent variable $z$ given sample $x$; while $p_{\theta}(x\vert z)$ is a decoder which generates samples $x$ given latent variable $z$.
 
 
**2.4.1 Kullback-Leibler (KL) Divergence:** 

The Kullback–Leibler (KL) divergence (also called relative entropy and I-divergence[1]), denoted $D_{KL}(P∥Q)$ is a type of statistical distance: a measure of how much a model probability distribution $Q$ is different from a true probability distribution $P$. 

Mathematically, it is defined as


$${\displaystyle D_{\text{KL}}(P\parallel Q)=\sum _{x\in {\mathcal {X}}}P(x)\,\log {\frac {P(x)}{Q(x)}}{\text{.}}}$$

This term ensures that the learned distribution in the latent space is close to a prior distribution, usually a standard Gaussian. It acts as a regularizer, preventing the model from encoding too much information in the latent space and ensuring smoothness in the latent space.


In Variational Autoencoders (VAEs), we use the reverse KL divergence (also called the KL divergence from the approximate posterior to the prior, $\text{KL}\left(Q_{\phi}(z\vert x)\Vert p_{\theta}(z)\right)$ rather than the forward KL divergence $\text{KL}\left(p_{\theta}(z)\Vert Q_{\phi}(z\vert x)\right)$ because the forward KL divergence would require sampling from the true (but unknown) posterior $p_\theta(z|x)$ which is intractable.


**2.4.2 Variational/Evidence Lower Bound (ELBO)**

The Variational Lower Bound $\mathcal{L}(\theta, \phi; \mathbf{x})$, also called the Evidence Lower Bound (ELBO), is the objective function used to train Variational Autoencoders (VAEs). It provides a tractable approximation to the intractable true posterior $p_\theta(\mathbf{z}|\mathbf{x})$.


As shown by [@zhai2018autoencoder], a variational lower bound of the marginal log-likelihood $\log p_{\theta}(x)$ can be derived as follows:

\begin{align*} \log p_{\theta}(x) & = \int\nolimits_{z} Q_{\phi}(z\vert x)\log p_{\theta}(x)dz\\ & = \int\nolimits_{z} Q_{\phi}(z\vert x)\log\left(\frac{p_{\theta}(x, z) Q_{\phi}(z\vert x)}{p_{\theta}(z\vert x) Q_{\phi}(z\vert x)}\right)dz\\ & = \int\nolimits_{z} Q_{\phi}(z\vert x)\left(\log\left(\frac{Q_{\phi}(z\vert x)}{p_{\theta}(z\vert x)}\right)+\log\left(\frac{p_{\theta}(x, z)}{Q_{\phi}(z\vert x)}\right)\right)dz\\ & =\text{KL}(Q_{\phi}(z\vert x)\Vert p_{\theta}(z\vert x))+\mathrm{E}_{Q_{\phi}(z\vert x)} \left[\frac{\log p_{\theta}(x, z)}{\log Q_{\phi}(z\vert x)}\right] \end{align*}


Since $\text{KL}\left(Q_{\phi}(z\vert x)\Vert p_{\theta}(z\vert x)\right)\geq 0$, variational lower bound $L(\theta,\phi;x)$ can be formulated as:

\begin{align*} \log\ p_{\theta}(x) & \geq E_{Q_{\phi}(z\vert x)}[\log p_{\theta}(x, z)-\log Q_{\phi}(z\vert x)]\\ & = E_{Q_{\phi}(z\vert x)}[\log p_{\theta}(z)-\log p_{\theta}(x\vert z)-\log Q_{\phi}(z\vert x)]\\ & =-KL(Q_{\phi}(z\vert x)\Vert p_{\theta}(z))+ E_{Q_{\phi}(z\vert x)}[\log p_{\theta}(z\vert x)]\\ & =L(\theta,\phi;x) \end{align*}



Since KL divergence $\geq 0$:
$$
\log p_\theta(\mathbf{x}) \geq \mathcal{L}(\theta, \phi; \mathbf{x})
$$
Maximizing $\mathcal{L}(\theta, \phi; \mathbf{x})$ indirectly maximizes 
$\log p_\theta(\mathbf{x})$

The ELBO is defined as:

$$
\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right] - \text{KL}\left( q_\phi(\mathbf{z}|\mathbf{x}) \parallel p_\theta(\mathbf{z}) \right)
$$

where:

- $\mathbf{x}$: Observed data (e.g., an image)
- $\mathbf{z}$: Latent variable
- $q_\phi(\mathbf{z}|\mathbf{x})$: Approximate posterior (encoder)
- $p_\theta(\mathbf{x}|\mathbf{z})$: Likelihood (decoder)
- $p_\theta(\mathbf{z})$: Prior over latents (typically $\mathcal{N}(0, I)$)


The VAE loss function has two components:

**The Reconstruction Term:**
$$
\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right]
$$
This measures how well the decoder reconstructs $\mathbf{x}$ from $\mathbf{z}$
and it is similar to an autoencoder's reconstruction loss. This can also be written as:

$$L_\text{MSE}(\theta,\phi) = \displaystyle\frac{1}{N}\sum_{i=1}^{N}{\left(x_i -f_\theta (g_\phi (x_i))\right)^2}$$

**The Regularization Term (KL-divergence):**
$$
\text{KL}\left( q_\phi(\mathbf{z}|\mathbf{x}) \parallel p_\theta(\mathbf{z}) \right)
$$
This penalizes deviations from the prior and ensures a structured latent space. This can also be written as:

$$L_\text{KL}[G(Z_{\mu}, Z_{\sigma})  |  \mathcal{N}(0, 1)] = -0.5 * \sum_{i=1}^{N}{1 + \log(Z_{\sigma_{i}^2}) - Z_{\mu_{i}}^2 -  Z_{\sigma_{i}}^2}$$

- $L_\text{KL}$ represents the KL divergence loss.
- $G(Z_{\mu}, Z_{\sigma})$ is the Gaussian distribution defined by the encoder’s outputs $Z_{\mu}$ (mean) and $Z_{\sigma}$ (standard deviation).
- $\mathcal{N}(0, 1)$ is the standard normal distribution.
- The formula calculates the difference between the encoder’s distribution and the standard normal distribution for each sample and sums these differences.


The combined VAE loss is a weighted sum of the reconstruction and KL divergence losses:

$$\mathcal{L}_\text{VAE} = \mathcal{L}_\text{recon} + \mathcal{L}_\text{KL}$$

::: {.figure}
![](images/loss.png){width=100%}

<div style="text-align: left; position: relative;">
  <span class="caption">Figure 5: VAE Objective</span>
  <span style="position: absolute; right: 0;">*Source:* [@noauthor_reparameterization_nodate]</span>
</div>
:::


We want to maximize the (log-)likelihood of generating real data (that is $p_\theta(\mathbf{x})$ and also minimize the difference between the real and estimated posterior distributions.


### 2.5 Generative Adversarial Network (GAN)

Generative Adversarial Networks (GANs) help machines to create new, realistic data by learning from existing examples. As [@goodfellow2014generative] explained, the GAN model consists of a Generator (G) and a Discriminator (D). The generative G captures the data distribution, and the discriminative D estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake.

This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined  by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples.

::: {.figure}
![](images/GAN-model.png){width=100%}

<div style="text-align: left; position: relative;">
  <span class="caption">Figure 6: Generative Adversarial Network </span>
  <span style="position: absolute; right: 0;">*Source:* [@zhu2017unpaired] </span>
</div>
:::

The Generator's  objective is to produce samples that the discriminator classifies as real. It tries to minimize this loss:

$$
J_{G} = -\frac{1}{m} \sum_{i=1}^{m} \log D(G(z_i))
$$

where:

- $J_{G}$ measures how well the generator is fooling the discriminator
- $G(z_i)$ is the generated sample from random noise $z_i$
- $D(G(z_i))$ is the discriminator's estimated probability that the generated sample is real


The discriminator's loss function is given by:

$$
J_{D} = - \frac{1}{m} \sum_{i=1}^{m} \log D(x_i) - \frac{1}{m} \sum_{i=1}^{m} \log \left(1 - D(G(z_i))\right)
$$

where:

- **$J_{D}$** measures how well the discriminator classifies real and fake samples.
- **$x_i$** is a real data sample.
- **$G(z_i)$** is a fake sample generated from random noise $z_i$.
- **$D(x_i)$** is the discriminator’s probability that $x_i$ is real.
- **$D(G(z_i))$** is the discriminator’s probability that the fake sample is real.


**Minimax GAN Loss**

The fundamental objective of a GAN is given by the minimax game:

$$
\min_{G} \max_{D} V(G, D) = \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]
$$

where:

- **$G$** is the generator network (learns to generate fake data).
- **$D$** is the discriminator network (learns to distinguish real vs. fake).
- **$p_{\text{data}}(x)$** is the true data distribution.
- **$p_{z}(z)$** is the noise distribution (typically Gaussian or uniform).
- **$D(x)$** is the discriminator's probability that input $x$ is real.
- **$D(G(z))$** is the discriminator's probability that the generator's output is real.
- **$\mathbb{E}$**  is the Probability expectation

The generator ($G$) tries to minimize $\log(1 - D(G(z)))$ (i.e., make fake data indistinguishable).

The discriminator ($D$) tries to maximize:

  - $\log D(x)$ (correctly classify real data), and
  
  - $\log (1 - D(G(z)))$ (correctly reject fake data).

### 2.6 Cycle GAN
CycleGAN builds upon the GAN framework, employing two GANs instead of one. Unlike traditional GANs, CycleGAN takes user-provided images as input rather than random noise, improving user control, resolution, and output quality.

The goal of CycleGAN is to learn mapping functions between two domains $X$ and $Y$ given training samples $\{x_i\}_{i=1}^N$ where $x_i \in X$ and $\{y_j\}_{j=1}^M$ where $y_j \in Y$. As [@zhu2017unpaired] showed, CycleGAN model includes two mapping functions $G: X → Y$ and $F: Y → X$ , and associated adversarial discriminators $Dₓ$ and $Dᵧ$. The discriminator,$Dᵧ$ encourages the generator $G$ to translate $X$ into outputs indistinguishable from domain $Y$ ; in the same way, $Dₓ$ encourages the generator $F$ to translate $Y$ into outputs indistinguishable from domain $X$.


::: {.figure}
![](images/cycleGAN-model.png){width=100%}

<div style="text-align: left; position: relative;">
  <span class="caption">Figure 7: Cycle GAN Model </span>
  <span style="position: absolute; right: 0;">*Source:* [@zhu2019brief] </span>
</div>
:::

The objective contains two types of losses (for the two mapping functions and their corresponding discriminators):
- adversarial losses for matching the distribution of generated images to the data distribution in the target domain; and 
- cycle consistency losses to prevent the learned mappings $G$ and $F$ from contradicting each other.

::: {.figure}
![](images/cycle-loss.png){width=100%}

<div style="text-align: left; position: relative;">
  <span class="caption">Figure 8: Cycle GAN with Cycle Consistency Loss </span>
  <span style="position: absolute; right: 0;">*Source:* [@zhu2019brief] </span>
</div>
:::

**Adversarial Losses**

For the mapping function $G : X → Y$ and its discriminator $Dᵧ$, we express the objective as:

$$
L_{\text{GAN}}(G, D_Y, X, Y) = \mathbb{E}_{y \sim p_{\text{data}}(y)} [\log D_Y(y)] + \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log(1 - D_Y(G(x)))]
$$
Similarly, for the mapping function $F : Y → X$ and its discriminator $Dₓ$, we express the objective as:

$$
L_{\text{GAN}}(F, D_X, X, Y) = \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D_X(x)] + \mathbb{E}_{y \sim p_{\text{data}}(y)} [\log(1 - D_X(G(y)))]
$$

**Cycle Consistency Losses**

To further regularize the mappings, two cycle consistency losses are introduced that capture the intuition that if we translate from one domain to the other and back again we should arrive at where we started. 

Below, the Figure 8: (b) shows forward cycle-consistency loss: for each image x from domain $X$, the image translation cycle should be able to bring $x$ back to the original image, i.e.,$x → G(x) → F(G(x)) ≈ x$. Similarly, the Figure 8: (c) shows  backward cycle-consistency loss: for each image $y$ from domain $Y$ , $G$ and $F$ should also satisfy backward cycle consistency:$y → F(y) → G(F(y)) ≈ y$


\begin{align*}
L_{\text{cyc}}(G, F) &= \mathbb{E}_{x \sim p_{\text{data}}(x)} [\|F(G(x)) - x\|_1] \\
&\quad + \mathbb{E}_{y \sim p_{\text{data}}(y)} [\|G(F(y)) - y\|_1]
\end{align*}
    
**Full Objective**

The full objective can be written as:
\begin{align*}
L(G, F, D_X, D_Y) &= L_{\text{GAN}}(G, D_Y, X, Y) \\
&\quad + L_{\text{GAN}}(F, D_X, Y, X) \\
&\quad + \lambda L_{\text{cyc}}(G, F),
\end{align*}
where $\lambda$ controls the relative importance of the two objectives. We aim to solve:

\begin{align*}
G^*, F^* = \arg \min_{G,F} \max_{D_X,D_Y} L(G, F, D_X, D_Y).
\end{align*}


### 2.7 VAE-GAN

By combining  a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective.

We collapse the VAE decoder and the GAN generator into one by letting them share parameters and training them jointly. VAE-GAN learns a probabilistic latent space via encoder $q_{\phi}(z\vert x)$.


::: {.figure}
![](images/VAE-GAN.png){width=100%}

<div style="text-align: left; position: relative;">
  <span class="caption">Figure 9: VAE-GAN Model </span>
  <span style="position: absolute; right: 0;">*Source:* [@razghandi2022variational] </span>
</div>
:::


The loss function $\mathcal{L}$ for training VAE-GAN model is defined as a combination of VAE loss $\mathcal{L}_{\text{VAE}}$ and GAN loss $\mathcal{L}_{\text{GAN}}$


$$\mathcal{L} = \mathcal{L}_{\text{VAE}} + \mathcal{L}_{\text{GAN}}$$



\begin{align*}
\mathcal{L}_{\text{VAE-GAN}}(x) &= \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{\text{KL}}(q_\phi(z|x) \parallel p(z))}_{\text{VAE Loss (Reconstruction + KL Divergence)}} \\
&\quad + \lambda \underbrace{\left(\mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim q_\phi(z|x)}[\log(1 - D(G(z)))]\right)}_{\text{GAN Adversarial Loss}}
\end{align*}

where:

- $x$: Input data sample  
- $z$: Latent variable  
- $q_\phi(z|x)$: approximate posterior of the Encoder with parameters $\phi$  
- $p_\theta(x|z)$: Decoder (generative model) with parameters $\theta$  
- $p(z)$: Prior distribution over latent space typically $\mathcal{N}(0,I)$ 
- $D_{\text{KL}}(q \parallel p)$: Kullback-Leibler divergence between distributions $q$ and $p$
- $D$: Discriminator network  
- $G$: Generator/decoder network shared with VAE decoder  
- $\beta$: Weight for KL divergence term which controls disentanglement  
- $\lambda$: Weight for adversarial loss (balances VAE and GAN objectives)
- $\mathbb{E}$: Probability Expectation 

### 2.8 VAE-CycleGAN

The composite architecture of VAE integrated with CycleGAN is given below.

::: {.figure}
![](images/VAE-CycleGAN.png){width=100%}

<div style="text-align: center; position: relative;">
  <span class="caption">Figure 10: VAE-CycleGAN Model </span>
</div>
:::

- ${(x_1, ..., x_n)}$: Input data distribution for the Generator 1 ($x\to y$)
- ${(y_1, ..., y_n)}$: Input data distribution for the Generator 2 ($y \to x$)

- ${(p_1,..., p_n)}$ and ${(q_1,..., q_n)}$: Translated/generated images  

The loss function $\mathcal{L}$ for training VAE-CycleGAN model is defined as a four key components:

$$
\mathcal{L}_{\text{Total}} = \underbrace{\mathcal{L}_{\text{VAE}}}_{\text{Probabilistic Encoding}} + \underbrace{\mathcal{L}_{\text{GAN}}}_{\text{Adversarial}} + \underbrace{\lambda_{\text{cyc}}\mathcal{L}_{\text{cycle}}}_{\text{Domain Consistency}} + \underbrace{\lambda_{\text{id}}\mathcal{L}_{\text{identity}}}_{\text{Content Preservation}}
$$


**VAE Loss** 

The VAE loss consists of two main components:

1. Reconstruction Loss: It measures how well the decoder reconstructs the input data. Since we are not reconstructing the input with a decoder and the generated output is not paired (no deterministic real value to compare with), we do not consider this.
2. KL Divergence Loss: It regularizes the latent space by encouraging the encoder's distribution $q(z|x)$ to match the prior $p(z)$ (usually standard normal)
\begin{align*}
\mathcal{L}_{\text{VAE}}^X &= \mathbb{E}_{q_ϕ(z|x)}\left[\log p_θ(x|z)\right] - \beta D_{\text{KL}}(q_ϕ(z|x) \parallel p(z)) \\
\mathcal{L}_{\text{VAE}}^Y &= \mathbb{E}_{q_ϕ(z|y)}\left[\log p_θ(y|z)\right] - \beta D_{\text{KL}}(q_ϕ(z|y) \parallel p(z))
\end{align*}

**Adversarial Loss** 

It is standard GAN loss making generated images indistinguishable from real ones

\begin{align*}
\mathcal{L}_{\text{GAN}}^{X→Y} &= \mathbb{E}_{y \sim p_{\text{data}}(y)}[\log D_ϕ(y)] + \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log(1 - D_ϕ(G_θ(x))] \\
\mathcal{L}_{\text{GAN}}^{Y→X} &= \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D_ϕ(x)] + \mathbb{E}_{y \sim p_{\text{data}}(y)}[\log(1 - D_ϕ(G_θ(y))]
\end{align*}


**Cycle consistency Loss**

It ensures forward and backward cycles reconstruct original images

\begin{align*}
\mathcal{L}_{\text{cycle}} &= \mathbb{E}_{x \sim p_{\text{data}}(x)}\left[\|x - G_{θ_2}(G_{θ_1}(x))\|_1\right] \\
&\quad + \mathbb{E}_{y \sim p_{\text{data}}(y)}\left[\|y - G_{θ_1}(G_{θ_2}(y))\|_1\right]
\end{align*}

**Identity Loss**

It ensures the generator to preserve the input's identity when the input is already from the target domain.
 
\begin{align*}
\mathcal{L}_{\text{identity}} &= \mathbb{E}_{x \sim p_{\text{data}}(x)}\left[\|G_θ(x) - x\|_1\right] \\
&\quad + \mathbb{E}_{y \sim p_{\text{data}}(y)}\left[\|G_θ(y) - y\|_1\right]
\end{align*}

**Total VAE-CycleGAN Loss**
\begin{align*}
\mathcal{L}_{\text{Total}} &= \mathcal{L}_{\text{VAE}}^X + \mathcal{L}_{\text{VAE}}^Y \\
&\quad + \lambda_{\text{GAN}}(\mathcal{L}_{\text{GAN}}^{X→Y} + \mathcal{L}_{\text{GAN}}^{Y→X}) \\
&\quad + \lambda_{\text{cycle}}\mathcal{L}_{\text{cycle}} \\
&\quad + \lambda_{\text{id}}\mathcal{L}_{\text{identity}}
\end{align*}

- $\lambda_{cyc}$: Weight for cycle consistency loss
- $\lambda_{identity}$: Weight for identity loss
- $\theta_1$: $\theta$ parameter for Generator 1  
- $\theta_2$: $\theta$ parameter for Generator 2 
- all the other parameters are similar to VAE-GAN losses


### 2.9 Metrics ###

The key metrics used in image processing and generative models are Mean Squared Error (MSE),  Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) and Fréchet Inception Distance (FID)

**Mean Squared Error (MSE)**

 Measures the average squared difference between the predicted (generated) image and the ground truth (real) image.
$$
MSE = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2
$$


where:

- $y_i$ = true pixel value
- $\hat{y}_i$ = predicted pixel value
- $N$ = total number of pixels

**Peak Signal-to-Noise Ratio (PSNR)**

A logarithmic measure of image quality based on MSE, relative to the maximum possible pixel value (e.g., 255 for 8-bit images):


$$
PSNR = 10 \cdot \log_{10}\left(\frac{MAX^2}{MSE}\right)
$$


where:

- $MAX$ = maximum possible pixel value (255 for 8-bit images)

Higher PSNR indicates better quality (common in compression, restoration tasks). Values typically range from 20 dB (poor) to 50 dB (excellent).

**Structural Similarity Index (SSIM)**

The Structural Similarity Index (SSIM) is a perceptual metric that quantifies the similarity between two images by comparing their luminance, contrast, and structure. Unlike pixel-based metrics like MSE and PSNR, SSIM aligns better with human perception.

SSIM Formula: For two image patches x and y,


$$
SSIM(x,y) = \frac{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}
$$

where:
- $\mu_x, \mu_y$ = average pixel intensities (luminance)
- $\sigma_x, \sigma_y$ = standard deviations (contrast)
- $\sigma_{xy}$ = covariance (structure)
- $C_1, C_2$ = small constants to avoid division by zero

**Fréchet Inception Distance (FID)**
Measures the similarity between two sets of images (real vs. generated) using features extracted from a pre-trained Inception-v3 model.

Computes the Fréchet distance between multivariate Gaussians fitted to the feature distributions:


$$
FID = \|\mu_r - \mu_g\|^2 + Tr(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
$$

where:

- $\mu_r, \mu_g$ = mean features
- $\Sigma_r, \Sigma_g$ = covariance matrices


The lower the FID, the better the quality and diversity of generated images (closer to real data). FID is typically a standard metric for GANs. It correlates well with human perception and captures high-level semantics (not just pixels). 


### 2.10 Convergence

*Why does a neural network trained with cycle loss converge to a single, invertible mapping?*

We begin by defining the spaces and mappings relevant to the cycle-consistent neural network framework.

Let $X$ denote the input domain, where each element $x \in X$ represents one of $n$ distinct items (e.g., images), as determined by the dimensionality of the input data. Let $Y$ be the target output domain, where each $y \in Y$ likewise corresponds to one of $n$ distinct items, consistent with the output data dimension.

Let $f : X \to Z$ denote the forward mapping implemented by the neural network, where $Z$ is an intermediate representation space. Let $g : Z \to X$ be the inverse mapping used to reconstruct the original input from the network's output. In the context of a model trained with cycle-consistency loss, we assume the following two properties:

1. $\forall x \in X, \exists g$ such that $g(f(x)) = x \quad (1)$ 

2. $Z = Y \quad (2)$ 

Equation (1) ensures existence of a cycle-consistent mapping. Equation (2) states that the intermediate representation space $Z$ is equivalent to the target output domain $Y$, thereby implying that the network effectively learns a mapping from $X$ to $Y$.

Given that $f$ is bijective by Equation (1) and that $|X| = |Y| = n$ by Equation (2), it follows that there exist $n!$ possible one-to-one mappings (i.e., permutations) between elements of $X$ and $Y$. Although many such bijective mappings exist in theory, the cycle-consistency loss biases the network toward converging on a single, consistent, and invertible transformation that minimizes the reconstruction error.

Let $f_\theta$ denote the forward neural network (parameterized by weights $\theta$) which maps inputs from domain $X$ to outputs in domain $Y$. Let $g_\theta$ denote the inverse neural network, also parameterized by $\theta$, that attempts to reconstruct the original input from the output of $f_\theta$. By Equation (1) and $|X| = |Y| = n$, note that $g = f^{-1}$.

The cycle-consistency loss $\mathcal{L}(\theta)$ is defined as the expected reconstruction error between the original input $x$ and its reconstruction $g_\theta(f_\theta(x))$, measured using the squared $L2$ norm:

$$\mathcal{L}(\theta) = \mathbb{E}_{x\sim X}\left[\|g_\theta(f_\theta(x)) - x\|_2^2\right] \quad (3)$$

The optimal parameters $\theta^*$ are obtained by minimizing the cycle-consistency loss:

$$\theta^* = \arg\min_\theta \mathcal{L}(\theta) \quad (4)$$

We assume the following about the solution ($\theta^*$) landscape:

1. No local minima exist (i.e., network optimizer will never be stuck at a local minima)
2. There exists a unique $\theta^*$ such that $\mathcal{L}(\theta^*) < \epsilon$ for some $\epsilon \in \mathbb{R}^\quad (5)$

Solution uniqueness is enforced by the neural network's inherent incompleteness: the neural network cannot perfectly reconstruct $x$, i.e.,

$$\mathcal{L}(\theta) > 0 \quad \forall \theta \quad (6)$$

Since exact recovery is impossible, the model cannot satisfy cycle-consistency for any parameterization/mapping. So, the model will choose the lowest $\theta^*$ as given in Equation (4) for convergence.

Under these assumptions, gradient descent will thus converge to a unique solution $\theta^*$ with corresponding invertible mappings $(f_{\theta^*}, g_{\theta^*})$ between domains $X$ and $Y$.


## 3 Implementation 

Our task involves: 

 (1) converting satellite/aerial photos to map-like representations (and vice versa) without requiring paired training data and 
 (2) get a realistic translations or generation of new satellite photos (or maps) that are close to the original real aerial photos (or maps)

The VAE-CycleGAN is well-suited for this because:

- Satellite images and maps are unpaired and are not exact pixel-aligned pairs.

- Maps require structural consistency like roads, buildings etc. which VAEs help preserve.

- CycleGAN ensures realistic outputs while maintaining geometric integrity.

The dataset consists of satellite photos and images. The number of samples in the dataset are 1096, each image has the structure of (C, H, W) where:

- C: the number of input channels
- H, W:  the height and width of the images
- all the images have a dimension of (3, 64, 64)


The configuration parameters used for training the model are given in the code below.

<div class="code-toggle">
  <button class="code-toggle-btn">Configuration Parameters</button>
  <div class="code-content collapsed">

```python

    'epoch': 0,
    'n_epochs': 601 ,                     # max channels
    'dataset_name': 'maps',               # Squashed latent dimension used for VAE
    'batch_size': 16, 
    'lr': 0.0002,
    'b1': 0.5,                            #'leaky_relu',  # 'relu', 'leaky_relu', 'sin'
    'b2': 0.999, 
    'decay_epoch': 2,
    'n_cpu': 8, 
    'img_height': 64,
    'img_width': 64,
    'channels': 3,
    'sample_interval': 500,  #100
    'checkpoint_interval': 1,
    'n_residual_blocks': 1, 
    'lambda_cyc': 10.0,
    'lambda_id': 5.0,                     # 0.0, # 1.0, #0.0001, # 1e-6, #1e-6,
    'latent_dim': 256,                    # Squashed latent dimension used for VAE
    'n_layers': 2,                        # Number of layers in the generator
    'lambda_kl': 1e-05,                   # KL divergence loss weight for VAE

```
  </div>
</div>
   

### Training Procedure ####

#### 3.1 Input Image transformations####

A series of image transformations such as data augmentations and preprocessing steps are typically used when preparing images for training a deep learning model.  Before training the VAE-CycleGAN model, the following transformations are made to the data:

- Resize: Each image is resized 12%  larger than the target height while maintaining aspect ratio.
- Crop : To add variability in the training data, the resized images are randomly cropped to the target height and width dimensions specified in model configuration parameters 
- Flip: Randomly flipped the image horizontally with a 50% probability to double the training data effectively (data augmentation).
- To Tensor: Converted the image from PIL format to a PyTorch Tensor. This method also scaled the  pixel values from [0, 255] to [0, 1]
- Normalize: Normalized the tensor image with mean 0.5 and standard deviation 0.5 for each channel (RGB) to effectively scale the pixel values from [0, 1] to [-1, 1]


<div class="code-toggle">
  <button class="code-toggle-btn">Image transformations</button>
  <div class="code-content collapsed">

```python

# Image transformations
transforms_ = [
    transforms.Resize(int(model_config['img_height'] * 1.12), Image.BICUBIC),
    transforms.RandomCrop((model_config['img_height'], model_config['img_width'])),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
]

```
  </div>
</div>
  
  

#### 3.2 Satellite → Map Translation (X → Y) ####


![](images/EncoderDecoder.png){width=100%}


The VAE helps in learning a probabilistic latent space that captures meaningful representations of the input data.

**Encoder**

The Encoder E extracts the latent features from satellite images. The high dimensional image with 3 input channels (RGB) and with a spacial dimension of 64x64 is reduced to a latent dimension of 256 channels with a spacial resolution of 16x16. The encoder is used for dimensionality reduction and for feature learning.
  

<div class="code-toggle">
  <button class="code-toggle-btn">Downsample</button>
  <div class="code-content collapsed">
  
```python

class Downsample(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, factor=2):
        super(Downsample, self).__init__()
        
        self.conv = nn.Conv2d(in_channels, out_channels // factor**2, kernel_size=kernel_size, stride=stride, padding=padding)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.factor = factor
        assert out_channels % factor**2 == 0
        assert in_channels * factor**2 % out_channels == 0
        self.group_size = int(in_channels * factor**2 // out_channels)

    def forward(self, x):
        y = self.conv(x)
        y = F.pixel_unshuffle(y, self.factor)

        x = F.pixel_unshuffle(x, self.factor)
        x = x.view(x.shape[0], self.out_channels, self.group_size, *x.shape[2:])
        x = x.mean(dim=2)  # Average over the group dimension

        return y + x  # Add the downsampled skip connection

```
  </div>
</div>


Added to down sampling, the encoder also contains the initial convolution and the residuals blocks which are explained in detail  in the section *4 Encoder * below. 

**Probabilistic sampling **

As explained in the *Methods 2.2* section, the encoder produces a Gausssian distribution’s parameters (mean and variance), and the actual latent representation is sampled from this distribution.

Satellite image $x$ → Encoder $E(x)$ → Latent $z$



<div class="code-toggle">
  <button class="code-toggle-btn">Probabilistic sampling </button>
  <div class="code-content collapsed">
  
```python
        # normal distribution
        groups = 1
        self.fc_mu = GroupedLinear(out_features, latent_dim, groups=groups, skip=True) # * self.latent_img_size**2
        self.fc_logvar = GroupedLinear(out_features, latent_dim, groups=groups, skip=False)
        self.fc_decode = GroupedLinear(latent_dim, out_features, groups=groups, skip=True)

```
  </div>
</div>


**Decoder**

From a $Z$ distribution sample, the decoder $G(z)$ generates maps $\hat{y}$.  The low dimensional image with 256 input channels (RGB) and with a spacial dimension of 16x16 is enlarged to an output of 3 channels with a spacial resolution of 64x64. The translated/generated output should match with one of the images in the Y domain. The decoder in the generator 1 translates satellite photos to maps whereas the decoder in the generator 2 translated maps to satellite pictures. 

Added to up-sampling, the decoder also contains the residuals blocks, instance normalization and reflection padding. The purpose and the functionality of these blocks and their implementation are discussed in the section *4 Encoder * below. 

Latent $z$  → Decoder $G(z)$ →  $\hat{y}\ $ (generated/fake map)


<div class="code-toggle">
  <button class="code-toggle-btn">Upsample </button>
  <div class="code-content collapsed">
  
```python

class Upsample(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, factor=2):
        super(Upsample, self).__init__()

        self.conv = nn.Conv2d(in_channels, out_channels * factor**2, kernel_size=kernel_size, stride=stride, padding=padding)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.factor = factor
        assert out_channels * factor**2 % in_channels == 0, f"out_channels * factor**2 ({out_channels * factor**2}) must be divisible by in_channels ({in_channels})"
        self.repeats = out_channels * factor**2 // in_channels

    def forward(self, x):
        z = self.conv(x) + x.repeat_interleave(self.repeats, dim=1) # conv w skip
        return F.pixel_shuffle(z, self.factor)
        

```
  </div>
</div>



<div class="code-toggle">
  <button class="code-toggle-btn">Decoder </button>
  <div class="code-content collapsed">
  
```python

        # Decoder
        decoder = []
        for _ in range(num_residual_blocks):
            decoder += [ResidualBlock(out_features)]

        # Upsampling
        for _ in range(n_layers):
            out_features //= 2
            decoder += [
                Upsample(in_features, out_features, kernel_size=3, padding=1),
                nn.InstanceNorm2d(out_features),
                nn.ReLU(inplace=True),
            ]
            in_features = out_features

        # Output layer
        decoder += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]

        self.decoder = nn.Sequential(*decoder)

```
  </div>
</div>


**Generators**

The Generator 1 consists of encoder, probabilistic sampling and a decoder that converts satellite images to latent distribution to maps.

Satellite image $x$ → Encoder $E(x)$ → Latent $z$  → Decoder $G(z)$ → $\hat{y}$ (generated Map image)


The Generator 2 consists of encoder, probabilistic sampling and a decoder that converts maps to latent distribution to satellite images.


Map image $y$ → Encoder $E(y)$ → Latent $z$  → Decoder $F(z)$ →  $\hat{x}$ (generated Satellite image)


**Discriminator D_Y ** 

Discriminator D_Y differentiates whether the generator map image  $\hat{y}$ is a real map or a fake map; accordingly updates the training parameters and gives an update to the Generator-X

D_Y outputs 0 if it detects that the generated map image is a fake and outputs 1 if it gets tricked(fooled) by the generator 1 and takes the generated map image as the real image.


<div class="code-toggle">
  <button class="code-toggle-btn">Discriminator </button>
  <div class="code-content collapsed">
  
```python

class Discriminator(nn.Module):
    def __init__(self, input_shape):
        super(Discriminator, self).__init__()

        channels, height, width = input_shape

        # Calculate output shape of image discriminator (PatchGAN)
        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)

        def discriminator_block(in_filters, out_filters, normalize=True):
            """Returns downsampling layers of each discriminator block"""
            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]
            if normalize:
                layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *discriminator_block(channels, 64, normalize=False),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(512, 1, 4, padding=1)
        )

    def forward(self, img):
        return self.model(img)


```
  </div>
</div>

#### 3.3 Map → Satellite Translation (Y → X) ####

The process explained in *section 3.2* is repeated for the generator-2 except that the input is changed from satellite photos to maps and the generated output are aerial photos instead of maps.

Map image $y$ → Encoder $E(y)$ → Latent $z$  → Decoder $F(z)$ →  $\hat{x}\ $ (generated/fake satellite image)


**Discriminator D_X ** 

Discriminator $D_X$ differentiates whether the generator satellite photo  $\hat{x}$ is a real aerial picture or a fake picture; accordingly updates the training parameters and gives an update to the Generator-Y.

$D_X$ outputs 0 if it detects that the generated satellite image is a fake and outputs 1 if it gets tricked(fooled) by the generator 2 and takes the generated satellite image as the real image. Discriminators ($D_X, D_Y$) ensure realism in both X and Y domains


#### 3.4 GAN loss ####

Loss GAN_XY is the mean square loss of the discriminator $D_Y$ output with the valid tensor vector of 1s'. Similarly, Loss GAN_YX is the mean square loss of the discriminator $D_X$ output with the valid tensor vector of 1s'.

GAN loss is the average of the GAN losses from both generators.

![](images/loss_GAN_XY.png){width=100%}


<div class="code-toggle">
  <button class="code-toggle-btn">GAN Loss </button>
  <div class="code-content collapsed">
  
```python

        # GAN loss
        fake_B = G_AB(real_A)
        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)
        fake_A = G_BA(real_B)
        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)

        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2

```
  </div>
</div>


#### 3.5 Cycle Consistency ####

To maintain the cycle consistency between the input images and the generated images, the output from Generator 1 is given as input to the Generator 2 and the output from the Generator 2 is compared with the original input. In other words, the first generator takes the input as the satellite images and gives the output as fake maps. The second generator takes the fake maps as input and generates fake satellite images. The L1 loss function is used to find the error between the generated fake satellite images with that of original satellite images.

Similarly, when we input maps to the Generator 2 the output are fake satellite photos. These fake aerial pictures are input to Generator 1 which generates fake maps.  The L1 loss function is used to find the error between the generated fake maps with that of original maps.

Cycle consistency ensures $F(G(E(x))) \approx \ x$ (reconstruct original satellite image). Similarly, $G(F(y)) \approx \ y$ (reconstruct original map).

![](images/loss_cycle_X_Y.png){width=100%}

<div class="code-toggle">
  <button class="code-toggle-btn">Cycle Consistency Loss </button>
  <div class="code-content collapsed">
  
```python

        # Cycle loss
        recov_A = G_BA(fake_B)
        loss_cycle_A = criterion_cycle(recov_A, real_A)
        recov_B = G_AB(fake_A)
        loss_cycle_B = criterion_cycle(recov_B, real_B)

        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2

```
  </div>
</div>


#### 3.6 Identity Loss ####

The generator 1 ($G_{XY}$) translates (generates) an image from the $X$ domain to the $Y$ domain. This means, for a given satellite image input, the $G_{XY}$ generates a map image. If a map image is given as input to the generator $G_XY$ then it should still preserve the input's identity and generate a map image from the latent distribution.

Similarly, when a satellite image is given as input to the generator  $G_{YX}$, then the generator should preserve the satellite image identity and translate it to another satellite image. 

The L1 loss measures the error between the generated images and the real images.

![](images/loss_identity_XY.png){width=100%}


<div class="code-toggle">
  <button class="code-toggle-btn">Identity Loss </button>
  <div class="code-content collapsed">
  
```python
    # Identity loss
        loss_id_A = criterion_identity(G_BA(real_A), real_A)
        loss_id_B = criterion_identity(G_AB(real_B), real_B)

        loss_identity = (loss_id_A + loss_id_B) / 2
```
  </div>
</div>


**Kullback-Leibler Divergence (KL) loss**

The KL-Divergence ensures the encoded distributions stay close to $N(0,1)$ but not identical. The encoder maps each input $x$ to a mini Gaussian distribution in latent space, defined by mean $\mu(x)$ and variance $\sigma^2(x)$. The KL Divergence Loss helps these mini-distributions $q (z|x)$ stay "well-behaved" relative to the prior $p(z)$

- If $\sigma^2_j$  → 0 ($j$ denotes the input sample), the latent dimension $z_j$ becomes deterministic with no randomness, defeating the purpose of a probabilistic VAE.
- If $\mu_j$ for each input grows too large, the latent space becomes unstructured, harming generation.
- If  $\sigma^2_j$  → $\infty$, the mini distributions overlap too much and the latent dimensions become noisy and uninformative.

Therefore, KL-divergence loss acts as a balancing act to prevent mini distributions from collapsing, from drifting too far, and from overlapping too much. This loss helps in producing realistic outputs from sampling the latent distribution, avoids the mode collapse and helps in smooth interpolation between latent points and meaningful translations. 


<div class="code-toggle">
  <button class="code-toggle-btn">KL-Divergence </button>
  <div class="code-content collapsed">
  
```python

    def kld_loss(self, mu, logvar):
        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return kld / mu.size(0)


    ### During Generator training
    if self.training:  # Only calculate KL divergence loss during training
        kl_loss = self.kld_loss(mu, logvar)
        wandb.log({"kl_loss": kl_loss.item()})  # Log KL divergence loss to Weights & Biases
        (kl_loss * self.lambda_kl).backward(retain_graph=True)  # Retain graph for backpropagation


```
  </div>
</div>


**Total Loss**

<div class="code-toggle">
  <button class="code-toggle-btn">Total Loss </button>
  <div class="code-content collapsed">
  
```python

# Total loss
loss_G = loss_GAN + model_config['lambda_cyc'] * loss_cycle  + model_config['lambda_id'] * loss_identity

```
  </div>
</div>

There's no VAE reconstruction loss since we are not reconstructing the input data using a decoder ; we are translating the input image ($x$, satellite photo from the $X$ domain) to a map image($y$) in another domain ($Y$). The model also considers unpaired images; therefore we cannot have a VAE reconstruction error. The KL-Divergence loss is added to the total loss above to get the complete objective. 



### 4 Encoder implementation ###

The detailed process and the VAE Encoder is as follows: 

**1. Initial Convolution:** 

This includes a Reflection Padding followed by a Convolution, Instance Normalization and a ReLU layer.
  
- Reflection Padding: It pads the input tensor by reflecting/mirroring values at the boundaries (instead of       padding with zeros or other constants). For a padding size of n, it mirrors the n nearest pixels along the      edges. This helps in preserving the image structure and avoids edge artifacts (unlike zero-padding).
      
- Convolution layer: Using a kernel_size of 7, this layer is used for feature extraction. The input image with
3 features is convoluted to an output image with 64 features. 
      
- Instance Normalization: Unlike BatchNorm, which normalizes across batches, InstanceNorm normalizes each 
channel in each sample independently. For input shape (N, C, H, W), it computes mean/variance per (C, H, W)
slice. Instance normalization helps for stable training; it is crucial for image generation and works well
with small batch sizes.
      
- ReLU : Introduces non-linearity after normalization.

<div class="code-toggle">
  <button class="code-toggle-btn">Reflect_Conv_Normalize</button>
  <div class="code-content collapsed">
  
```python

    self.block = nn.Sequential(
        nn.ReflectionPad2d(1),
        nn.Conv2d(in_features, in_features, 3),
        nn.InstanceNorm2d(in_features),
        nn.ReLU(inplace=True), 
        nn.ReflectionPad2d(1),
        nn.Conv2d(in_features, in_features, 3),
        nn.InstanceNorm2d(in_features),
    )

```
  </div>
</div>

  
There's no spatial compression yet; channels increase from 3 → 64. The input image (3, 64,64) is changed to an output (64, 64, 64).


**2. Downsampling Blocks:**

The downsampling module combines convolution, pixel unshuffle, and a skip connection to reduce spatial resolution while preserving information. 
  
- Convolution: This layer reduces channel dimension or extracts features before pixel unshuffle. It maps input 64 channels → output 16 channels. This compression is for efficiency before expensive spatial operations.
  
- Pixel Unshuffle: Rearranges spatial dimensions into channels. Pixel Shuffle (F.pixel_shuffle) and Pixel Unshuffle (F.pixel_unshuffle) are efficient operations for upsampling and downsampling in neural networks, often used as alternatives to transposed convolutions (for upsampling) and strided convolutions (for downsampling).

  - Pixel unshuffle decreases spatial resolution while increasing channel depth. It acts as the inverse of Pixel Shuffle. This is more stable than strided convolution and avoids aliasing.
  
  - For the input with dimensions ( C, H x r, W x r) converts to (C * /$r^2$, H/r, W/r). 
  
  - We used 2 layers of down sampling with a downscale factor of 2. 
   
  - Each Downsample layer reduces spatial size by 2× and doubles channels:

  - First Downsample: (64, 64, 64) → (128, 32, 32)

  - Second Downsample: (128, 32, 32) → (256, 16, 16)

- Skip Connection: 
  
  - A skip connection (or residual connection) is a pathway that allows the input to bypass one or more layers and be added directly to the output. This concept was popularized by **add link ResNet (2015)** and is widely used to improve gradient flow avoiding vanishing gradients and helps preserve input information.

  - A skip connection ensures the original input contributes to the output (residual learning) and stabilizes training 
 
  - During downsampling, skip connection processes the input x(in the code above) separately to match the shape of the convoluted and pixel unshuffled output y. 
 
  - Uses pixel unshuffle and channel averaging to align dimensions of the x with the output y (please check the code)
  

<div class="code-toggle">
  <button class="code-toggle-btn">Downsample</button>
  <div class="code-content collapsed">
  
```python

class Downsample(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, factor=2):
        super(Downsample, self).__init__()
        
        self.conv = nn.Conv2d(in_channels, out_channels // factor**2, kernel_size=kernel_size, stride=stride, padding=padding)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.factor = factor
        assert out_channels % factor**2 == 0
        assert in_channels * factor**2 % out_channels == 0
        self.group_size = int(in_channels * factor**2 // out_channels)

    def forward(self, x):
        y = self.conv(x)
        y = F.pixel_unshuffle(y, self.factor)

        x = F.pixel_unshuffle(x, self.factor)
        x = x.view(x.shape[0], self.out_channels, self.group_size, *x.shape[2:])
        x = x.mean(dim=2)  # Average over the group dimension

        return y + x  # Add the downsampled skip connection

```
  </div>
</div>


**3. Residual Blocks:**

  - Residual Blocks consists of two reflection paddings, two convolutions, two instance normalization, a Relu activation layer for non-linearity and a skip connection. The purpose of reflection padding, instance norm and skip connections are explained earlier.
  
  - While avoiding vanishing gradients during back propagation, and learning the residual (difference) between the input and the output with skip connection, the residual blocks ensures stable training even with many layers.
  



<div class="code-toggle">
  <button class="code-toggle-btn">Residual Block</button>
  <div class="code-content collapsed">
  
```python
class ResidualBlock(nn.Module):
    def __init__(self, in_features):
        super(ResidualBlock, self).__init__()

        self.block = nn.Sequential(
            nn.ReflectionPad2d(1),
            nn.Conv2d(in_features, in_features, 3),
            nn.InstanceNorm2d(in_features),
            nn.ReLU(inplace=True), 
            nn.ReflectionPad2d(1),
            nn.Conv2d(in_features, in_features, 3),
            nn.InstanceNorm2d(in_features),
        )

    def forward(self, x):
        return x + self.block(x)

```
  </div>
</div>

  
  - We used 2 layers of residual blocks
  
  - These maintain the same dimensions of the output (256, 16, 16) after downsampling.



**Metrics**

The Frechet Inception Distance, Structural Similarity Index Measure, Mean Squared Error, and  Peak Signal Noise Ratio are calculated between the translated/generated images and the original images.

<div class="code-toggle">
  <button class="code-toggle-btn">Metrics </button>
  <div class="code-content collapsed">
  
```python

# Initialize metrics
fid = FrechetInceptionDistance(feature=2048).to(device)
ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)

mse = MeanSquaredError().to(device)
psnr = PeakSignalNoiseRatio().to(device)

# Function to calculate metrics between real and fake images
@torch.no_grad()

def calculate_metrics(real, fake):
    """Calculate various metrics between real and fake images"""
    metrics = {}
    
    # Denormalize images (assuming they're in [-1, 1] range)
    real_denorm = (real + 1) / 2  # Scale to [0, 1]
    fake_denorm = (fake + 1) / 2
    
    # Reset metrics
    mse.reset()
    psnr.reset()
    ssim.reset()
    fid.reset()
    
    # Calculate metrics
    metrics['mse'] = mse(real_denorm, fake_denorm).item()
    metrics['psnr'] = psnr(real_denorm, fake_denorm).item()
    metrics['ssim'] = ssim(real_denorm, fake_denorm).item()
    
    # For FID, we need uint8 images in 0-255 range
    real_uint8 = (real_denorm * 255).byte()
    fake_uint8 = (fake_denorm * 255).byte()
    
    # Update FID
    fid.update(real_uint8, real=True)
    fid.update(fake_uint8, real=False)
    metrics['fid'] = fid.compute().item()

    # Convert all metrics to Python floats
    return {k: float(v) if hasattr(v, 'item') else float(v) for k, v in metrics.items()}
    
    return metrics

```
  </div>
</div>


## 4 Results


### 4.1 CycleGAN Results###

Below are the results of the images generated from a CycleGAN model - with out probabilistic sampling from the latent space.

In Figure (a) below, the first row shows the input satellite pictures followed by generated maps for the satellite pictures in the second row. The third row shows the input maps followed by generated satellite pictures in the fourth row. 

In Figure (b) below, the first row shows the input satellite pictures followed by generated/fake maps for the satellite pictures in the second row. The third row shows the generated satellite pictures from the fake maps.The fourth row shows the generated maps from the fake satellite pictures.


<div id="tbl-ae-simple" class="full-width-table">
<table style="width:100%">
  <tr>
    <th>Aerial Views to Maps (X $\to$ Y $\&$  Y $\to$ X )</th>
    <th>Aerial Views to Maps (X $\to$ Y $\to$  X'$\to$ Y')</th>
  </tr>
  <tr>
    <td><img src="images/Results/CycleGAN_a2m_603_X_Y.png" style="width:100%"></td>
    <td><img src="images/Results/CycleGAN_a2m_603_X_Y_X_Y.png" style="width:100%"></td>
  </tr>
  <tr>
    <td>(a)</td>
    <td>(b)</td>
  </tr>
</table>
<p>CycleGAN: Satellite to Maps Transformation</p>
</div>

### 4.2 VAE-CycleGAN Results###


Below are the results of the images generated from a Variation Autoencoder CycleGAN model - with probabilistic sampling from the latent space.

<div id="tbl-vae-simple" class="full-width-table">
<table style="width:100%">
  <tr>
    <th>Aerial Views to Maps (X $\to$ Y $\&$  Y $\to$ X )</th>
    <th>Aerial Views to Maps (X $\to$ Y $\to$  X'$\to$ Y')</th>
  </tr>
  <tr>
    <td><img src="images/Results/VAE_cycleGAN_a2m_603_X_Y.png" style="width:100%"></td>
    <td><img src="images/Results/VAE_cycleGAN_a2m_603_X_Y_X_Y.png" style="width:100%"></td>
  </tr>
  <tr>
    <td>(a)</td>
    <td>(b)</td>
  </tr>
</table>
<p>VAE-CycleGAN: Satellite to Maps Transformation</p>
</div>

![](images/Results/losses_1.png){width=100%}

![](images/Results/losses_2.png){width=100%}

### 4.3 Metrics###

**Compression Rate **

Compression rate measures how much the input is reduced in spatial dimensions while increasing channel depth. It's calculated as:


$$
\text{Compression Rate} = \frac{\text{Latent Spatial Size}}{\text{Input Spatial Size}} \times \frac{\text{Input Channels}}{\text{Final Channels}}
$$
 
The latent representation in the VAE-CycleGAN model is a flat vector (not spatial), Therefore, we the spatial reduction factor and channel expansion/reduction are described separately.

**Spatial Compression**

Input Spatial Size: 64×64=4096 pixels.

Final Encoder Output: 16×16=256 pixels.


$$
\text{Spatial Compression Factor}: \frac{64 \times 64}{16 \times 16} = 16 \times
$$

*(The image is compressed to 1/16th of its original spatial area.)*

**Channel Expansion**

Input Channels: 3 (RGB).

Final Encoder Channels: 256.


$$
\text{Channel Expansion Factor}: \frac{256}{3} \approx 85.33 \times
$$

**AE-CycleGAN**

![](images/Results/CycleGAN_a2m_metrics.png){width=100%}

**VAE-CycleGAN**

![](images/Results/VAE_cycleGAN_a2m_metrics.png){width=100%}


## 5 Conclusions

1. 
2. 

## 6 References 

