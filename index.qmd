---
title: "VAE-CycleGAN Project - Summer 2025"
subtitle: "Image Translation/Generation"
author: "Sridevi Autoor (Advisor: Dr. Cohen) "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
engine: knitr  # Forces R as the default engine    
jupyter: python3  # Uses Python (requires Jupyter kernel)
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72


---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

Image-to-image translation is a fundamental task in computer vision, enabling applications such as style transfer, domain adaptation, and photo enhancement. The goal is to learn a mapping function that transforms an input image from a source domain (e.g., daytime photos) into a corresponding output in a target domain (e.g., nighttime photos). While recent advances in deep generative models have significantly improved translation quality, key challenges remain—particularly in unpaired settings, where aligned training data is unavailable, and in controllable generation, where users desire fine-grained manipulation of outputs.


#### Background and Limitations of Existing Approaches

Deep generative models have traditionally struggled with intractable probabilistic computations, especially in maximum likelihood estimation, and have found it difficult to effectively use piecewise linear units in generative settings. Early models such as Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs) depend on computationally expensive Markov Chain Monte Carlo (MCMC) methods, while Variational Autoencoders (VAEs) require approximate inference. Other techniques, like score matching and noise-contrastive estimation, impose strict constraints on how probability densities are specified.

Generative Adversarial Networks (GANs)[@goodfellow2014generative] address many of these challenges through an adversarial training framework. They eliminate the need for Markov chains, explicit likelihoods, or approximate inference. By framing learning as a minimax game between a generator ($G$) and discriminator ($D$), GANs learn a loss that tries to classify if the output image is real or fake, while simultaneously training a generative model to minimize this loss. GANs enable efficient backpropagation-based optimization while producing high-fidelity samples.

Though GANs are powerful for generating realistic data, they suffer from a few drawbacks:

1. Mode Collapse: the generator may produce limited varieties of outputs instead of covering the full data distribution. For ex: We get the same painting output for a variety of input photos. 

2. Non-convergence: the competition between the generator ($G$) and discriminator ($D$) may never stabilize, causing persistent oscillation in losses.

3. Requires paired data: GAN-based approaches such as conditional GANs [@isola2017image], requires  paired examples ($x$, $y$) where the generator learns a mapping from input image $x$ to generate the output image $y$. However, obtaining such paired datasets is often impractical in real-world scenarios.


To overcome these limitations, unsupervised image-to-image translation models such as CycleGAN [@zhu2017unpaired] have been developed which employ two generator-discriminator pairs ($G:X→Y$ and $F:Y→X$) and implement cycle-consistency loss ($F(G(X)) ≈ X$ and vice versa) along with adversarial losses to enable stable unpaired image translation. Nevertheless, CycleGAN has its own drawbacks:

1. There’s no explicit latent space within the generator (or between the two generators) and therefore there is no control over output. The mapping between domains is learned implicitly by the generators. We can't easily specify style, color, or semantic features of the output. A vanilla encoder-decoder CycleGAN (without VAE) is just a deterministic autoencoder-style model. For example, given an  image of a horse, it will always generate the same image of a zebra.

2. There's limited output diversity. Deterministic mappings in the latent space within a generator inhibit multimodal generation. For example, translating a cloudy sky to multiple sunset styles is not possible.  

3. For a given number of $n$ distinct inputs ($\forall x_i \in X,\ i \in \{1,\ldots,n\}$) that belong to the $X$ domain, the model may fail to translate/generate a single output ($y_i \in Y,\ i \in \{1,\ldots,n\}$) out of the possible $n!$ permutations (more in the *Methods* section).  


#### Our Approach: VAE-CycleGAN

We propose VAE-CycleGAN, a hybrid framework that integrates Variational Autoencoders (VAEs) into CycleGAN. Incorporating a VAE into CycleGAN (or replacing parts of CycleGAN with VAE components) combines the adversarial and cycle-consistent training of CycleGAN with the probabilistic latent space technique of VAEs. The advantages of VAE-CycleGAN over the standard CycleGAN or a simple encoder-decoder CycleGAN are:

1. Instead of a fixed latent code within the generator, the VAE encodes inputs as mini distribution means, enabling sampling. A VAE-based CycleGAN enforces a structured latent distribution (e.g., Gaussian) via the Kullback-Leibler divergence (KL) loss, making interpolation and manipulation easier (e.g., for attribute editing). 

2. While CycleGAN deterministically maps one input to one output, a VAE can model multimodality by sampling different latent codes $z$ and decode them back to generate different outputs, thus capturing uncertainty and variability in the data. For ex: The generator can produce multiple zebra images (may be different stripe patterns, change in backgrounds, poses etc.) for the same photo of a horse.


By unifying VAEs and CycleGAN, this project develops a framework for bi-directional translation, where the model:

1. learns to map images from a source domain ($X$, e.g., pictures) to a target domain ($Y$, e.g., paintings) without requiring paired training data

2. enforces a probabilistic latent via the KL divergence loss 

3. ensures realistic and reversible translations (via CycleGAN) with adversarial and cycle-consistency 

4. preserves input fidelity during domain transfers with VAE reconstruction loss.  


## Methods
## Results
## Conclusions
## References
