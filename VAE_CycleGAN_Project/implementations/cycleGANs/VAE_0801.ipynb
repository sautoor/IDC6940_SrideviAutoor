{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1505857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "import wandb\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image \n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from model_general_cyclegan_vae import *\n",
    "\n",
    "from datasets import *\n",
    "from utils import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920726ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Sets the seed for reproducibility across different components.\"\"\"\n",
    "    random.seed(seed)  # Python's built-in random module\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    # torch.cuda.manual_seed(seed)  # PyTorch CUDA (single GPU)\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorch CUDA (multi-GPU)\n",
    "    # os.environ['PYTHONHASHSEED'] = str(seed)  # For hash-based operations\n",
    "\n",
    "    # # Enforce deterministic behavior for CuDNN (can impact performance)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Enforce deterministic algorithms for all PyTorch operations (can raise errors if not available)\n",
    "    # torch.use_deterministic_algorithms(True) # Uncomment if strict determinism is required and all ops support it\n",
    "\n",
    "# Example usage:\n",
    "seed_value = 42\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403eb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model configuration\n",
    "# This configuration is used to define the parameters for training the CycleGAN with VAE.\n",
    "# The values can be adjusted based on the specific requirements of the training process.\n",
    "\n",
    "model_config = {\n",
    "    'epoch': 0,\n",
    "    'n_epochs': 601 ,# max channels\n",
    "    'dataset_name': 'maps', # dataset\n",
    "    'batch_size': 16, #0,\n",
    "    'lr': 0.0002,\n",
    "    'b1': 0.5,   #'leaky_relu',  # 'relu', 'leaky_relu', 'sin'\n",
    "    'b2': 0.999,  ##4, 3\n",
    "    'decay_epoch': 2,\n",
    "    'n_cpu': 8, # 1.0, #0.0001, # 1e-6, #1e-6,\n",
    "    'img_height': 64,\n",
    "    'img_width': 64,\n",
    "    'channels': 3,\n",
    "    'sample_interval': 500,  #100\n",
    "    'checkpoint_interval': 1,\n",
    "    'n_residual_blocks': 1, #1\n",
    "    'lambda_cyc': 10.0,\n",
    "    'lambda_id': 5.0, # 0.0, # 1.0, #0.0001, # 1e-6, #1e-6,\n",
    "    'latent_dim': 256, # Squashed latent dimension used for VAE\n",
    "    'n_layers': 2, #2 # Number of layers in the generator\n",
    "    'lambda_kl': 1e-05, # KL divergence loss weight for VAE\n",
    "    'run_name': 'VAE_e601', #pixel_shuffle_sm_ldim128_l4_r2_pos_enc\n",
    "\n",
    "    'saved_models_file': \"saved_models_VAE\",\n",
    "    'images_file' : \"images_VAE\",\n",
    "    'images_cycle_file': \"images_VAE_cycle\",\n",
    "    'test_results_file': \"test_results_VAE\",\n",
    "\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5f16f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msautoor\u001b[0m (\u001b[33msautoor-university-of-west-florida\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mom-wsl/ml/Project/cyclegan_eriklindernoren/implementations/GENERAL_cycleGAN/wandb/run-20250801_192320-1dhphyp1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sautoor-university-of-west-florida/CycleGAN/runs/1dhphyp1' target=\"_blank\">VAE_e601</a></strong> to <a href='https://wandb.ai/sautoor-university-of-west-florida/CycleGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sautoor-university-of-west-florida/CycleGAN' target=\"_blank\">https://wandb.ai/sautoor-university-of-west-florida/CycleGAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sautoor-university-of-west-florida/CycleGAN/runs/1dhphyp1' target=\"_blank\">https://wandb.ai/sautoor-university-of-west-florida/CycleGAN/runs/1dhphyp1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sautoor-university-of-west-florida/CycleGAN/runs/1dhphyp1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7def1b783a90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"CycleGAN\", name=model_config['run_name'], config=model_config)\n",
    "# wandb.watch(model, log=\"all\", log_freq=model_config['log_interval'])\n",
    "\n",
    "# model.train_loop(train_dataloader, val_dataloader, epochs=model_config['epochs'], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e806acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset path: ../../data/maps\n",
      "Number of samples in dataset: 1096\n",
      "First sample shape: torch.Size([3, 64, 64])\n",
      "First sample shape: torch.Size([3, 64, 64])\n",
      "Compression Ratio: 16.00:1\n",
      "GPU Memory Allocated: 0.06 GB\n",
      "[Epoch 600/601] [Batch 68/69] [D loss: 0.030714] [G loss: 1.843898, adv: 0.750222, cycle: 0.075210, identity: 0.068315] ETA: 0:00:00.084649{'mse': 0.03864389285445213, 'psnr': 13.663229942321777, 'ssim': 0.20813539624214172, 'fid': 364.8481140136719}\n",
      "{'mse': 0.03551651909947395, 'psnr': 13.803985595703125, 'ssim': 0.04967421293258667, 'fid': 269.29742431640625}\n",
      "{'mse': 0.0128769027069211, 'psnr': 18.893125534057617, 'ssim': 0.5429456233978271, 'fid': 324.21075439453125}\n",
      "{'mse': 0.0037293164059519768, 'psnr': 24.25999641418457, 'ssim': 0.46013012528419495, 'fid': 241.2958526611328}\n",
      "{'mse': 0.0337069109082222, 'psnr': 12.233099937438965, 'ssim': 0.10632374882698059, 'fid': 396.5418701171875}\n",
      "{'mse': 0.017027586698532104, 'psnr': 17.66900062561035, 'ssim': 0.4755973517894745, 'fid': 369.2305908203125}\n",
      "Testing complete! Results saved to test_results/\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>kl_loss</td><td>▅▂▁▁▁▁▄▁▁▁▁▂▁▁▁▁▅▁▁▁▁▅▁▂▁▁▁▁▅▁▅▂▁▁█▁▁▁▁▁</td></tr><tr><td>loss_GAN</td><td>▁▃▇▄▆▄▄▆▅▆▅▇▆▆▇█▅▅▇▅▅▆▆▇▆▅▆▆▅▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>loss_GAN_AB</td><td>▃▄▅▁▂▂█▅▄▆▆▅▅▇▅▇▅▅▅▄▅▅▅▆▅▅▅▆▅▅▅▅▆▅▅▅▅▅▅▄</td></tr><tr><td>loss_GAN_BA</td><td>▃▂▁▂▂▅▄▃▃▃▄▄▃▄▄▃▅▅▆▅▄█▄▆▅▆▆▆▅▅▄▄▆▅▅▅▆▄▅▄</td></tr><tr><td>loss_cycle</td><td>▆█▆▅▅▅▆▄▄▄▃▃▄▃▄▃▂▄▃▂▄▄▂▃▃▃▃▃▃▂▂▂▂▁▃▃▂▃▂▂</td></tr><tr><td>loss_cycle_A</td><td>█▇▇▅▇▄▅▆▄▆▃▆█▄▄▅▃▃▃▄▅▃▄▄▃▁▄▃▂▂▆▅▄▃▄▂▃▄▅▄</td></tr><tr><td>loss_cycle_B</td><td>█▆▄▄▃▃▃▃▄▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▁▂▁▂▁▂▂▂▁▁▂▂▁▁▁▁</td></tr><tr><td>loss_identity</td><td>█▅▅▃▄▃▂▄▃▃▂▃▂▂▂▃▂▂▂▃▂▂▂▃▂▂▂▁▂▂▁▂▂▁▂▂▁▁▂▁</td></tr><tr><td>loss_identity_A</td><td>█▇▆█▆▅▅▇▅▆▅▃▃▄▄▃▃▄▂▂▃▄▃▄▄▄▂▂▃▂▂▃▃▅▂▃▂▁▁▂</td></tr><tr><td>loss_identity_B</td><td>█▄▃▂▃▂▄▃▂▂▂▂▂▂▃▂▂▁▁▂▂▃▂▂▂▁▂▂▂▁▂▂▁▁▂▂▁▁▁▂</td></tr><tr><td>total_loss_G</td><td>█▄▃▃▄▄▅▄▄▂▃▃▃▁▂▃▂▃▃▃▃▃▂▃▂▃▂▃▃▂▂▂▃▃▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>600</td></tr><tr><td>kl_loss</td><td>2820.98657</td></tr><tr><td>loss_GAN</td><td>0.75022</td></tr><tr><td>loss_GAN_AB</td><td>0.79831</td></tr><tr><td>loss_GAN_BA</td><td>0.70213</td></tr><tr><td>loss_cycle</td><td>0.07521</td></tr><tr><td>loss_cycle_A</td><td>0.11018</td></tr><tr><td>loss_cycle_B</td><td>0.04024</td></tr><tr><td>loss_identity</td><td>0.06832</td></tr><tr><td>loss_identity_A</td><td>0.10279</td></tr><tr><td>loss_identity_B</td><td>0.03384</td></tr><tr><td>total_loss_G</td><td>1.8439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">VAE_e601</strong> at: <a href='https://wandb.ai/sautoor-university-of-west-florida/CycleGAN/runs/1dhphyp1' target=\"_blank\">https://wandb.ai/sautoor-university-of-west-florida/CycleGAN/runs/1dhphyp1</a><br> View project at: <a href='https://wandb.ai/sautoor-university-of-west-florida/CycleGAN' target=\"_blank\">https://wandb.ai/sautoor-university-of-west-florida/CycleGAN</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_192320-1dhphyp1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(f\"{model_config['images_file']}/epochs_{model_config['n_epochs']}\", exist_ok=True)\n",
    "os.makedirs(f\"{model_config['images_cycle_file']}/epochs_{model_config['n_epochs']}\", exist_ok=True)\n",
    "os.makedirs(f\"{model_config['saved_models_file']}/epochs_{model_config['n_epochs']}\", exist_ok=True)\n",
    "os.makedirs(f\"{model_config['test_results_file']}\", exist_ok=True)\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "input_shape = (model_config['channels'], model_config['img_height'], model_config['img_width'])\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G_AB = GeneratorResNet(input_shape, model_config['n_residual_blocks'], model_config['n_layers'], model_config['latent_dim'], model_config['lambda_kl'])\n",
    "G_BA = GeneratorResNet(input_shape, model_config['n_residual_blocks'], model_config['n_layers'], model_config['latent_dim'], model_config['lambda_kl'])\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)\n",
    "\n",
    "if cuda:\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()\n",
    "\n",
    "if model_config['epoch'] != 0:\n",
    "    # Load pretrained models\n",
    "    G_AB.load_state_dict(torch.load(f\"{model_config['saved_models_file']}/epochs_%s/G_AB_%d.pth\" % (model_config['n_epochs'], model_config['epoch'])))\n",
    "    G_BA.load_state_dict(torch.load(f\"{model_config['saved_models_file']}/epochs_%s/G_BA_%d.pth\" % (model_config['n_epochs'], model_config['epoch'])))\n",
    "    D_A.load_state_dict(torch.load( f\"{model_config['saved_models_file']}/epochs_%s/D_A_%d.pth\" % (model_config['n_epochs'], model_config['epoch'])))\n",
    "    D_B.load_state_dict(torch.load (f\"{model_config['saved_models_file']}/epochs_%s/D_B_%d.pth\" % (model_config['n_epochs'], model_config['epoch'])))\n",
    "\n",
    "\n",
    "else:\n",
    "    # Initialize weights\n",
    "    # G_AB.apply(weights_init_normal)\n",
    "    # G_BA.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=model_config['lr'], betas=(model_config['b1'], model_config['b2'])\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=model_config['lr'], betas=(model_config['b1'], model_config['b2']))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=model_config['lr'], betas=(model_config['b1'], model_config['b2']))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(model_config['n_epochs'], model_config['epoch'], model_config['decay_epoch']).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(model_config['n_epochs'], model_config['epoch'], model_config['decay_epoch']).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(model_config['n_epochs'], model_config['epoch'], model_config['decay_epoch']).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "# fake_A_buffer = ReplayBuffer()\n",
    "# fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Image transformations\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(model_config['img_height'] * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((model_config['img_height'], model_config['img_width'])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "dataset_path = \"../../data/%s\" % model_config['dataset_name']\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "# Create the dataset instance\n",
    "dataset = ImageDataset(dataset_path, transforms_=transforms_, unaligned=True)\n",
    "\n",
    "# Print dataset length and some samples info\n",
    "print(f\"Number of samples in dataset: {len(dataset)}\")\n",
    "if len(dataset) > 0:\n",
    "    print(f\"First sample shape: {dataset[0]['A'].shape if 'A' in dataset[0] else 'No A found'}\")\n",
    "    print(f\"First sample shape: {dataset[0]['B'].shape if 'B' in dataset[0] else 'No B found'}\")\n",
    "\n",
    "\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(\"../../data/%s\" % model_config['dataset_name'], transforms_=transforms_, unaligned=True),\n",
    "    batch_size=model_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=model_config['n_cpu'],\n",
    "    pin_memory=True,  # Faster data transfer to GPU\n",
    "    prefetch_factor=2,  # Preload next batches\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\"../../data/%s\" % model_config['dataset_name'], transforms_=transforms_, unaligned=False, mode=\"test\"),\n",
    "    batch_size=model_config['batch_size'], #5,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "# Add these near your other initialization code\n",
    "train_history = {\n",
    "    'D_loss': [],\n",
    "    'G_loss': [],\n",
    "    'G_GAN': [],\n",
    "    'G_cycle': [],\n",
    "    'G_identity': [],\n",
    "    'batches': []\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_cr(original_size, latent_size):\n",
    "    \"\"\"Calculate compression ratio between original and latent representation\"\"\"\n",
    "    # Assuming original_size is (channels, height, width)\n",
    "    original_elements = original_size[1] * original_size[2]\n",
    "   \n",
    "    # Calculate compression ratio\n",
    "    cr = original_elements / latent_size\n",
    "    return cr\n",
    "\n",
    "# You can call this with your image dimensions and latent dimension\n",
    "latent_shape =input_shape[1]//(2**(model_config['n_layers']))\n",
    "latent_shape = latent_shape**2  # Adjusted to match the latent dimension used in the model\n",
    "\n",
    "cr = calculate_cr(input_shape, latent_shape)\n",
    "print(f\"Compression Ratio: {cr:.2f}:1\")\n",
    "\n",
    "def plot_metric_curves(metrics, save_path=\"metric_curves.png\"):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot MSE\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(metrics['train']['mse'], label='Train MSE')\n",
    "    plt.plot(metrics['val']['mse'], label='Val MSE')\n",
    "    plt.title('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot PSNR\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(metrics['train']['psnr'], label='Train PSNR')\n",
    "    plt.plot(metrics['val']['psnr'], label='Val PSNR')\n",
    "    plt.title('PSNR')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot SSIM\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(metrics['train']['ssim'], label='Train SSIM')\n",
    "    plt.plot(metrics['val']['ssim'], label='Val SSIM')\n",
    "    plt.title('SSIM')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('SSIM')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot FID\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(metrics['train']['fid'], label='Train FID')\n",
    "    plt.plot(metrics['val']['fid'], label='Val FID')\n",
    "    plt.title('FID')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('FID')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curves(history, save_path=\"learning_curves.png\"):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot Generator losses\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history['batches'], history['G_loss'], label='Total G Loss')\n",
    "    plt.plot(history['batches'], history['G_GAN'], label='GAN Loss')\n",
    "    plt.plot(history['batches'], history['G_cycle'], label='Cycle Loss')\n",
    "    plt.plot(history['batches'], history['G_identity'], label='Identity Loss')\n",
    "    plt.title('Generator Learning Curves')\n",
    "    plt.xlabel('Batch Number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Discriminator loss\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history['batches'], history['D_loss'], label='D Loss', color='red')\n",
    "    plt.title('Discriminator Learning Curve')\n",
    "    plt.xlabel('Batch Number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Function to save colored images \n",
    "@torch.no_grad()  \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves generated samples in a 4x4 grid with clear labels\"\"\"\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "\n",
    "    try:\n",
    "        # Get a batch of test images\n",
    "        batch = next(iter(val_dataloader))\n",
    "\n",
    "        real_A = batch[\"A\"].cuda()      # Real A (Domain A)\n",
    "        real_B = batch[\"B\"].cuda()     # Real B (Domain B)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_B = G_AB(real_A)           # Fake B (A → B)\n",
    "            fake_A = G_BA(real_B)           # Fake A (B → A)\n",
    "            \n",
    "            cycle_A = G_BA(fake_B)           # Fake B → Cycle-Consistent A          \n",
    "            cycle_B= G_AB(cycle_A)           # cycle_A -> Cycle-Consistent B\n",
    "\n",
    "        # Denormalize images\n",
    "        def denorm(tensor):\n",
    "            return torch.clamp(tensor * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "        # Denormalize images for visualization\n",
    "        real_A = denorm(real_A[:4])\n",
    "        fake_B = denorm(fake_B[:4])\n",
    "        real_B = denorm(real_B[:4])\n",
    "\n",
    "        fake_A = denorm(fake_A[:4])        \n",
    "        cycle_A = denorm(cycle_A[:4])\n",
    "        cycle_B = denorm(cycle_B[:4])\n",
    "        \n",
    "\n",
    "        grid_rows1 = [\n",
    "            real_A,     # Row 1: Real A\n",
    "            fake_B,     # Row 2: Fake B (generated from real A)\n",
    "            real_B,     # Row 3: Real B\n",
    "            fake_A      # Row 4: Fake A (generated from real B)\n",
    "        ]\n",
    "        grid1 = torch.cat(grid_rows1, dim=0)\n",
    "        \n",
    "        # Save the grid\n",
    "\n",
    "        save_image(\n",
    "            grid1,\n",
    "            f\"{model_config['images_file']}/epochs_{model_config['n_epochs']}/{batches_done}.png\",\n",
    "            nrow=4,       # 4 columns (4 images per row)\n",
    "            normalize=False,\n",
    "            padding=2,\n",
    "            pad_value=1.0  # White padding between images\n",
    "        )\n",
    "      \n",
    "        grid_rows2 = [\n",
    "            real_A,      # Row 1: Real A\n",
    "            fake_B,      # Row 2: Fake B (A → B)\n",
    "            cycle_A,     # Row 3: Cycle-A (B → A cycle)\n",
    "            cycle_B      # Row 4: cycle-B (cycle A → cycle B)\n",
    "        ]\n",
    "        grid2= torch.cat(grid_rows2, dim=0)\n",
    "\n",
    "        #Save the grid\n",
    "        save_image(\n",
    "            grid2,\n",
    "            f\"{model_config['images_cycle_file']}/epochs_{model_config['n_epochs']}/{batches_done}.png\",\n",
    "            nrow=4,       # 4 columns (4 images per row)\n",
    "            normalize=False,\n",
    "            padding=2,\n",
    "            pad_value=1.0  # White padding between images\n",
    "        )\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving sample images: {str(e)}\")\n",
    "\n",
    "\n",
    "##Function to save colored images \n",
    "\n",
    "@torch.no_grad()  \n",
    "\n",
    "def save_generated_images(epoch_num):\n",
    "    \"\"\"Saves generated samples in a 4x4 grid with clear labels\"\"\"\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "\n",
    "    try:\n",
    "        # Get a batch of test images\n",
    "        batch = next(iter(val_dataloader))\n",
    "\n",
    "        real_A = batch[\"A\"].cuda()      # Real A (Domain A)\n",
    "        real_B = batch[\"B\"].cuda()     # Real B (Domain B)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_B = G_AB(real_A)           # Fake B (A → B)\n",
    "            fake_A = G_BA(real_B)           # Fake A (B → A)\n",
    "            \n",
    "            cycle_A = G_BA(fake_B)           # Fake B → Cycle-Consistent A          \n",
    "            cycle_B= G_AB(cycle_A)           # cycle_A -> Cycle-Consistent B\n",
    "\n",
    "        # Denormalize images\n",
    "        def denorm(tensor):\n",
    "            return torch.clamp(tensor * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "        # Denormalize images for visualization\n",
    "        _real_A = denorm(real_A[:4])\n",
    "        _fake_B = denorm(fake_B[:4])\n",
    "        _real_B = denorm(real_B[:4])\n",
    "\n",
    "        _fake_A = denorm(fake_A[:4])        \n",
    "        _cycle_A = denorm(cycle_A[:4])\n",
    "        _cycle_B = denorm(cycle_B[:4])\n",
    "        \n",
    "\n",
    "        grid_rows1 = [\n",
    "            _real_A,     # Row 1: Real A\n",
    "            _fake_B,     # Row 2: Fake B (generated from real A)\n",
    "            _real_B,     # Row 3: Real B\n",
    "            _fake_A      # Row 4: Fake A (generated from real B)\n",
    "        ]\n",
    "        grid1 = torch.cat(grid_rows1, dim=0)\n",
    "        \n",
    "        # Save the grid\n",
    "\n",
    "        save_image(\n",
    "            grid1,\n",
    "            f\"{model_config['images_file']}/epochs_{model_config['n_epochs']}/{epoch_num}.png\",\n",
    "            nrow=4,       # 4 columns (4 images per row)\n",
    "            normalize=False,\n",
    "            padding=2,\n",
    "            pad_value=1.0  # White padding between images\n",
    "        )\n",
    "      \n",
    "\n",
    "        # Arrange in 4x4 grid for cycle-consistency images:\n",
    "        # Row 1: Real A (4 images)\n",
    "        # Row 2: Fake B (A→B translations)\n",
    "        # Row 3: Cycle-Consistent A (fake B → A)\n",
    "        # Row 4: Fake A (B→A translations)\n",
    "        grid_rows2 = [\n",
    "            _real_A,      # Row 1: Real A\n",
    "            _fake_B,      # Row 2: Fake B (A → B)\n",
    "            _cycle_A,     # Row 3: Cycle-A (B → A cycle)\n",
    "            _cycle_B      # Row 4: cycle-B (cycle A → cycle B)\n",
    "        ]\n",
    "        grid2= torch.cat(grid_rows2, dim=0)\n",
    "\n",
    "        #Save the grid\n",
    "        save_image(\n",
    "            grid2,\n",
    "            f\"{model_config['images_cycle_file']}/epochs_{model_config['n_epochs']}/{epoch_num}.png\",\n",
    "            nrow=4,       # 4 columns (4 images per row)\n",
    "            normalize=False,\n",
    "            padding=2,\n",
    "            pad_value=1.0  # White padding between images\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving sample images: {str(e)}\")\n",
    "\n",
    "    return real_A, real_B, fake_A, fake_B, cycle_A, cycle_B\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "# At start of training\n",
    "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# dictionary to track epoch metrics\n",
    "epoch_metrics = {\n",
    "    'train': {'mse': [], 'psnr': [], 'ssim': [], 'fid': []},\n",
    "    'val': {'mse': [], 'psnr': [], 'ssim': [], 'fid': []}\n",
    "}\n",
    "\n",
    "for epoch in range(model_config['epoch'], model_config['n_epochs']):\n",
    "\n",
    "    # Set models to train mode\n",
    "    G_AB.train()\n",
    "    G_BA.train()\n",
    "    D_A.train()\n",
    "    D_B.train()\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Set model input\n",
    "        real_A =batch[\"A\"].to(device)  \n",
    "        real_B = batch[\"B\"].to(device) \n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(real_A.size(0), *D_A.output_shape, \n",
    "                  dtype=torch.float32, device=real_A.device)\n",
    "\n",
    "        fake = torch.tensor(np.zeros((real_A.size(0), *D_A.output_shape)), dtype=torch.float32, device='cuda', requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        # G_AB.train()\n",
    "        # G_BA.train()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = G_AB(real_A)\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "        fake_A = G_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recov_A = G_BA(fake_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "     \n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + model_config['lambda_cyc'] * loss_cycle  + model_config['lambda_id'] * loss_identity\n",
    "\n",
    "        # Log losses to wandb        \n",
    "        loss_dict = {\n",
    "            'loss_GAN_AB': loss_GAN_AB.item(),\n",
    "            'loss_GAN_BA': loss_GAN_BA.item(),\n",
    "            'loss_GAN': loss_GAN.item(),\n",
    "            'loss_identity_A': loss_id_A.item(),\n",
    "            'loss_identity_B': loss_id_B.item(),\n",
    "            'loss_identity': loss_identity.item(),\n",
    "            'loss_cycle_A': loss_cycle_A.item(),\n",
    "            'loss_cycle_B': loss_cycle_B.item(),\n",
    "            'loss_cycle': loss_cycle.item(),\n",
    "            'total_loss_G': loss_G.item(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        wandb.log(loss_dict)\n",
    "\n",
    "        # Backward pass\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator A\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_A(real_A), valid)     \n",
    "\n",
    "        # Fake loss \n",
    "        fake_A_ = G_BA(real_B) \n",
    "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "        \n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # backward pass\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator B\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "\n",
    "        # Fake loss \n",
    "        # fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "        fake_B_ = G_AB(real_A) # fake_B_buffer.push_and_pop(fake_B)\n",
    "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "       \n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = model_config['n_epochs'] * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "    \n",
    "        # Store losses\n",
    "        train_history['D_loss'].append(loss_D.item())\n",
    "        train_history['G_loss'].append(loss_G.item())\n",
    "        train_history['G_GAN'].append(loss_GAN.item())\n",
    "        train_history['G_cycle'].append(loss_cycle.item())\n",
    "        train_history['G_identity'].append(loss_identity.item())\n",
    "        train_history['batches'].append(batches_done)\n",
    "\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                model_config['n_epochs'],\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_GAN.item(),\n",
    "                loss_cycle.item(),\n",
    "                loss_identity.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # =============================================\n",
    "        \n",
    "   \n",
    "    if ((epoch + 1) % 10 == 0 or (epoch + 1) == model_config['epoch'] or (epoch + 1) == model_config['n_epochs']  or (epoch + 1) >(model_config['n_epochs'] -5)):\n",
    "        val_real_A, val_real_B, val_fake_A, val_fake_B, val_cycle_A, val_cycle_B = save_generated_images(epoch)  # Save sample images for this epoch\n",
    "   \n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "\n",
    "    # if model_config['checkpoint_interval'] != -1 and epoch % model_config['checkpoint_interval'] == 0:\n",
    "    if model_config['checkpoint_interval']!= -1 and (((epoch +1) %100 ==0) or (epoch +1) == model_config['n_epochs']):\n",
    "        # Save model checkpoints\n",
    "        torch.save(G_AB.state_dict(), f\"{model_config['saved_models_file']}/epochs_%s/G_AB_%d.pth\" % (model_config['n_epochs'], epoch))\n",
    "        torch.save(G_BA.state_dict(), f\"{model_config['saved_models_file']}/epochs_%s/G_BA_%d.pth\" % (model_config['n_epochs'], epoch))\n",
    "        torch.save(D_A.state_dict(), f\"{model_config['saved_models_file']}/epochs_%s/D_A_%d.pth\" % (model_config['n_epochs'], epoch))\n",
    "        torch.save(D_B.state_dict(), f\"{model_config['saved_models_file']}/epochs_%s/D_B_%d.pth\" % (model_config['n_epochs'], epoch))\n",
    "\n",
    "\n",
    "    # Clip gradients to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(G_AB.parameters(), 1.0)\n",
    "    torch.nn.utils.clip_grad_norm_(G_BA.parameters(), 1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "# Initialize metrics\n",
    "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "\n",
    "\n",
    "# Add these near your other metric initializations\n",
    "mse = MeanSquaredError().to(device)\n",
    "psnr = PeakSignalNoiseRatio().to(device)\n",
    "\n",
    "\n",
    "# Save metrics to file\n",
    "metrics_path = f\"{model_config['test_results_file']}/epochs_{model_config['n_epochs']}_metrics.txt\"\n",
    "\n",
    "# Function to calculate metrics between real and fake images\n",
    "# Modify your calculate_metrics function:\n",
    "@torch.no_grad()\n",
    "def calculate_metrics(real, fake):\n",
    "    \"\"\"Calculate various metrics between real and fake images\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "\n",
    "    \"\"\"Handle batch size mismatches by trimming to smaller size\"\"\"\n",
    "    min_batch = min(real.shape[0], fake.shape[0])\n",
    "    real = real[:min_batch]\n",
    "    fake = fake[:min_batch]\n",
    "\n",
    "    # Denormalize images (assuming they're in [-1, 1] range)\n",
    "    real_denorm = (real + 1) / 2  # Scale to [0, 1]\n",
    "    fake_denorm = (fake + 1) / 2\n",
    "    \n",
    "    # Reset metrics\n",
    "    mse.reset()\n",
    "    psnr.reset()\n",
    "    ssim.reset()\n",
    "    fid.reset()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics['mse'] = mse(real_denorm, fake_denorm).item()\n",
    "    metrics['psnr'] = psnr(real_denorm, fake_denorm).item()\n",
    "    metrics['ssim'] = ssim(real_denorm, fake_denorm).item()\n",
    "    \n",
    "    # For FID, we need uint8 images in 0-255 range\n",
    "    real_uint8 = (real_denorm * 255).byte()\n",
    "    fake_uint8 = (fake_denorm * 255).byte()\n",
    "    \n",
    "    # Update FID\n",
    "    fid.update(real_uint8, real=True)\n",
    "    fid.update(fake_uint8, real=False)\n",
    "    metrics['fid'] = fid.compute().item()\n",
    "\n",
    "    # Convert all metrics to Python floats\n",
    "    # return {k: float(v) if hasattr(v, 'item') else float(v) for k, v in metrics.items()}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "##############################\n",
    "# dictionary to track epoch metrics\n",
    "epoch_metrics = {\n",
    "    'train': {'mse': [], 'psnr': [], 'ssim': [], 'fid': []},\n",
    "    'val': {'mse': [], 'psnr': [], 'ssim': [], 'fid': []}\n",
    "}\n",
    "\n",
    "\n",
    "    # Initialize epoch metrics\n",
    "train_metrics_A = {'mse': 0, 'psnr': 0, 'ssim': 0, 'fid':0, 'count': 0}\n",
    "val_metrics_A =  {'mse': 0, 'psnr': 0, 'ssim': 0, 'fid': 0, 'count': 0}  \n",
    "\n",
    "train_metrics_B = {'mse': 0, 'psnr': 0, 'ssim': 0, 'fid':0, 'count': 0}\n",
    "val_metrics_B =  {'mse': 0, 'psnr': 0, 'ssim': 0, 'fid': 0, 'count': 0} \n",
    "\n",
    "train_metrics_B = {'mse': 0, 'psnr': 0, 'ssim': 0, 'fid':0, 'count': 0}\n",
    "val_metrics_B =  {'mse': 0, 'psnr': 0, 'ssim': 0, 'fid': 0, 'count': 0} \n",
    "\n",
    "train_metrics_A = calculate_metrics(real_A, fake_A)\n",
    "val_metrics_A = calculate_metrics(val_real_A, val_fake_A)\n",
    "\n",
    "train_metrics_B = calculate_metrics(real_B, fake_B)\n",
    "val_metrics_B = calculate_metrics(val_real_B, val_fake_B)\n",
    "\n",
    "cycle_metrics_A = calculate_metrics(real_A, val_cycle_A)\n",
    "cycle_metrics_B = calculate_metrics(real_B, val_cycle_B)\n",
    "\n",
    "\n",
    "print(train_metrics_A)\n",
    "print(val_metrics_A)\n",
    "\n",
    "print(train_metrics_B)\n",
    "print(val_metrics_B)\n",
    "\n",
    "print(cycle_metrics_A)\n",
    "print(cycle_metrics_B)\n",
    "\n",
    "# Save metrics to file\n",
    "metrics_path = f\"{model_config['test_results_file']}/epochs_{model_config['n_epochs']}_metrics.txt\"\n",
    "\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    # f.write(\"Epoch\\tTrain MSE\\tTrain PSNR\\tTrain SSIM\\tTrain FID\\tVal MSE\\tVal PSNR\\tVal SSIM\\tVal FID\\n\")\n",
    "    f.write(f\"Compression Ratio: {cr:.2f}:1\\n\\n\")\n",
    "\n",
    "    ################ TRAIN A and VAL A\n",
    "\n",
    "    f.write(\"Train_A and Val_A\\n\")\n",
    "    f.write(\"Epoch\\tTrain MSE\\tTrain PSNR\\tTrain SSIM\\tTrain FID\\tVal MSE \\tVal PSNR\\tVal SSIM\\tVal FID\\n\")\n",
    "\n",
    "    f.write(f\"{epoch}\\t\")\n",
    "    f.write(f\"{train_metrics_A['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_A['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_A['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_A['fid']:.2f}\\t\\t\")\n",
    "    \n",
    "\n",
    "    f.write(f\"{val_metrics_A['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{val_metrics_A['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{val_metrics_A['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{val_metrics_A['fid']:.2f}\\n\\n\")\n",
    "   \n",
    "    ####### TRAIN B and VAL B \n",
    "    f.write(\"Train_B and Val_B\\n\")\n",
    "    f.write(\"Epoch\\tTrain MSE\\tTrain PSNR\\tTrain SSIM\\tTrain FID\\tVal MSE \\tVal PSNR\\tVal SSIM\\tVal FID\\n\")\n",
    "    f.write(f\"{epoch}\\t\")\n",
    "    f.write(f\"{train_metrics_B['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_B['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_B['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_B['fid']:.2f}\\t\\t\")\n",
    "    \n",
    "\n",
    "    f.write(f\"{val_metrics_B['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{val_metrics_B['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{val_metrics_B['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{val_metrics_B['fid']:.2f}\\n\\n\")\n",
    "\n",
    "    ####### TRAIN A and CYCLE A\n",
    "    f.write(\"Train_A and Cycle_A\\n\")\n",
    "    f.write(\"Epoch\\tTrain MSE\\tTrain PSNR\\tTrain SSIM\\tTrain FID\\tCyc MSE \\tCyc PSNR\\tCyc SSIM\\tCyc FID\\n\")\n",
    "\n",
    "    f.write(f\"{epoch}\\t\")\n",
    "    f.write(f\"{train_metrics_A['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_A['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_A['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_A['fid']:.2f}\\t\\t\")\n",
    "    \n",
    "\n",
    "    f.write(f\"{cycle_metrics_A['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{cycle_metrics_A['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{cycle_metrics_A['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{cycle_metrics_A['fid']:.2f}\\n\\n\")\n",
    "\n",
    "    ############ TRAIN B and CYCLE B\n",
    "\n",
    "    f.write(\"Train_B and Cycle_B\\n\")\n",
    "    f.write(\"Epoch\\tTrain MSE\\tTrain PSNR\\tTrain SSIM\\tTrain FID\\tCyc MSE \\tCyc PSNR\\tCyc SSIM\\tCyc FID\\n\")\n",
    "    f.write(f\"{epoch}\\t\")\n",
    "    f.write(f\"{train_metrics_B['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_B['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_B['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{train_metrics_B['fid']:.2f}\\t\\t\")\n",
    "    \n",
    "\n",
    "    f.write(f\"{cycle_metrics_B['mse']:.4f}\\t\\t\")\n",
    "    f.write(f\"{cycle_metrics_B['psnr']:.2f}\\t\\t\")\n",
    "    f.write(f\"{cycle_metrics_B['ssim']:.4f}\\t\\t\")\n",
    "    f.write(f\"{cycle_metrics_B['fid']:.2f}\\n\\n\")\n",
    "\n",
    "                \n",
    " \n",
    "print(\"Testing complete! Results saved to test_results/\")\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
